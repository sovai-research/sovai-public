{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d585232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install uv && uv pip install sovai['full'] --system > output.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0a3ddf7-efcd-4c9c-947c-e0eb481f4d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Set up Notion credentials (hardcoded as per your request)\n",
    "NOTION_TOKEN = \"your_notion_token_here\"  # **Ensure this token is kept secure!**\n",
    "DATABASE_ID = \"your_database_id_here\"\n",
    "NOTION_VERSION = \"2022-06-28\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {NOTION_TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Notion-Version\": NOTION_VERSION,\n",
    "}\n",
    "\n",
    "def create_page(title, database_id, children):\n",
    "    \"\"\"\n",
    "    Creates a new page in the specified Notion database.\n",
    "\n",
    "    Args:\n",
    "        title (str): The title of the page.\n",
    "        database_id (str): The ID of the Notion database.\n",
    "        children (list): A list of block objects to include in the page.\n",
    "\n",
    "    Returns:\n",
    "        dict: The response from the Notion API.\n",
    "    \"\"\"\n",
    "    page_data = {\n",
    "        \"parent\": {\"database_id\": database_id},\n",
    "        \"properties\": {\n",
    "            \"Title\": {\n",
    "                \"title\": [\n",
    "                    {\n",
    "                        \"text\": {\n",
    "                            \"content\": title\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "        },\n",
    "        \"children\": children\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.notion.com/v1/pages\", headers=headers, json=page_data)\n",
    "    return response\n",
    "\n",
    "\n",
    "def find_page_by_title(database_id, title):\n",
    "    \"\"\"\n",
    "    Searches the Notion database for a page with the specified title.\n",
    "\n",
    "    Args:\n",
    "        database_id (str): The ID of the Notion database.\n",
    "        title (str): The title to search for.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: The page object if found, else None.\n",
    "    \"\"\"\n",
    "    query_url = f\"https://api.notion.com/v1/databases/{database_id}/query\"\n",
    "    query_data = {\n",
    "        \"filter\": {\n",
    "            \"property\": \"Title\",\n",
    "            \"title\": {\n",
    "                \"equals\": title\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(query_url, headers=headers, json=query_data)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to query database:\")\n",
    "        print(json.dumps(response.json(), indent=2))\n",
    "        return None\n",
    "\n",
    "    results = response.json().get(\"results\")\n",
    "    if results:\n",
    "        return results[0]  # Assuming titles are unique\n",
    "    return None\n",
    "\n",
    "\n",
    "def append_to_page(page_id, children):\n",
    "    \"\"\"\n",
    "    Appends new blocks to an existing Notion page.\n",
    "\n",
    "    Args:\n",
    "        page_id (str): The ID of the page to append to.\n",
    "        children (list): A list of block objects to append.\n",
    "\n",
    "    Returns:\n",
    "        dict: The response from the Notion API.\n",
    "    \"\"\"\n",
    "    append_url = f\"https://api.notion.com/v1/blocks/{page_id}/children\"\n",
    "    append_data = {\n",
    "        \"children\": children\n",
    "    }\n",
    "    response = requests.patch(append_url, headers=headers, json=append_data)\n",
    "    return response\n",
    "\n",
    "\n",
    "def build_content_from_dict(content_dict):\n",
    "    \"\"\"\n",
    "    Builds Notion content blocks from a dictionary.\n",
    "\n",
    "    Args:\n",
    "        content_dict (dict): A dictionary containing content definitions.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Notion block objects.\n",
    "    \"\"\"\n",
    "    children = []\n",
    "\n",
    "    # Add Heading\n",
    "    if \"heading\" in content_dict and content_dict[\"heading\"]:\n",
    "        children.append(\n",
    "            {\n",
    "                \"object\": \"block\",\n",
    "                \"type\": \"heading_2\",\n",
    "                \"heading_2\": {\n",
    "                    \"rich_text\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": {\n",
    "                                \"content\": content_dict[\"heading\"]\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Add Content\n",
    "    if \"content\" in content_dict and content_dict[\"content\"]:\n",
    "        children.append(\n",
    "            {\n",
    "                \"object\": \"block\",\n",
    "                \"type\": \"paragraph\",\n",
    "                \"paragraph\": {\n",
    "                    \"rich_text\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": {\n",
    "                                \"content\": content_dict[\"content\"]\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Add List Items (Bullet Points)\n",
    "    if \"list\" in content_dict and content_dict[\"list\"]:\n",
    "        list_blocks = build_bullet_list(content_dict[\"list\"])\n",
    "        children.extend(list_blocks)\n",
    "        \n",
    "    # Add URL as a Link\n",
    "    if \"url\" in content_dict and content_dict[\"url\"]:\n",
    "        children.append(\n",
    "            {\n",
    "                \"object\": \"block\",\n",
    "                \"type\": \"paragraph\",\n",
    "                \"paragraph\": {\n",
    "                    \"rich_text\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": {\n",
    "                                \"content\": content_dict[\"url\"],\n",
    "                                \"link\": {\"url\": content_dict[\"url\"]}\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    return children\n",
    "\n",
    "\n",
    "def build_bullet_list(items):\n",
    "    \"\"\"\n",
    "    Builds Notion bullet list blocks from a list of items.\n",
    "\n",
    "    Args:\n",
    "        items (list): A list of strings representing bullet points.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Notion bulleted list item block objects.\n",
    "    \"\"\"\n",
    "    bullet_blocks = []\n",
    "    for item in items:\n",
    "        bullet_blocks.append(\n",
    "            {\n",
    "                \"object\": \"block\",\n",
    "                \"type\": \"bulleted_list_item\",\n",
    "                \"bulleted_list_item\": {\n",
    "                    \"rich_text\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": {\n",
    "                                \"content\": item\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "    return bullet_blocks\n",
    "\n",
    "\n",
    "def build_children_from_sections(content_sections):\n",
    "    \"\"\"\n",
    "    Iterates through the content sections dictionary and builds the children blocks.\n",
    "\n",
    "    Args:\n",
    "        content_sections (dict): Dictionary containing all content sections.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Notion block objects.\n",
    "    \"\"\"\n",
    "    children = []\n",
    "    for key in sorted(content_sections.keys()):\n",
    "        section = content_sections[key]\n",
    "        section_blocks = build_content_from_dict(section)\n",
    "        children.extend(section_blocks)\n",
    "    return children\n",
    "\n",
    "\n",
    "def handle_page_creation_or_append(title, database_id, content_sections):\n",
    "    \"\"\"\n",
    "    Handles the logic to either create a new page or append content to an existing page.\n",
    "\n",
    "    Args:\n",
    "        title (str): The title of the page.\n",
    "        database_id (str): The ID of the Notion database.\n",
    "        content_sections (dict): Dictionary containing all content sections.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try: \n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    except:\n",
    "        current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    full_title = f\"{title} - {current_date}\"\n",
    "\n",
    "    # Build the content blocks\n",
    "    children = build_children_from_sections(content_sections)\n",
    "\n",
    "    # Check if the page already exists\n",
    "    existing_page = find_page_by_title(database_id, full_title)\n",
    "\n",
    "    if existing_page:\n",
    "        print(f\"Page '{full_title}' already exists. Appending new content to it.\")\n",
    "        page_id = existing_page[\"id\"]\n",
    "        response = append_to_page(page_id, children)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"New content appended successfully.\")\n",
    "            # Construct the page URL manually\n",
    "            # Note: Notion page URLs follow the format https://www.notion.so/{workspace}/{page_id}\n",
    "            # However, constructing the exact URL might require additional steps.\n",
    "            # Here, we'll provide a placeholder.\n",
    "            page_url = f\"https://www.notion.so/{page_id.replace('-', '')}\"\n",
    "            print(f\"View your page here: {page_url}\")\n",
    "        else:\n",
    "            print(\"Failed to append new content:\")\n",
    "            print(json.dumps(response.json(), indent=2))\n",
    "    else:\n",
    "        print(f\"Page '{full_title}' does not exist. Creating a new page with the new content.\")\n",
    "        response = create_page(full_title, database_id, children)\n",
    "        \n",
    "        # Handle the response\n",
    "        if response.status_code == 200:\n",
    "            page_url = response.json().get(\"url\", \"No URL returned\")\n",
    "            print(\"Page created successfully with the new content.\")\n",
    "            print(f\"View your page here: {page_url}\")\n",
    "        else:\n",
    "            print(\"Failed to create page:\")\n",
    "            print(json.dumps(response.json(), indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4b18b5-4c3f-4662-b1c5-3b9e07098441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sovai as sov\n",
    "import pandas as pd\n",
    "\n",
    "sov.token_auth(token=\"visit https://sov.ai/profile for your token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5df9537-d119-4b7e-a1ec-871d4012e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_meta = pd.read_parquet(\"data/tickers.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bde74d59-5308-4f0c-9601-3e827d8709d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition 1: market_cap>50 (Standard filter)\n",
      "\n",
      "Filtering Results:\n",
      "┌─────────────────┬─────────────────┬─────────────────┬─────────────────┐\n",
      "│ Step            │   Total Tickers │         Removed │            Left │\n",
      "┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼\n",
      "│ Initial         │          22,124 │               - │               - │\n",
      "│ Condition 1     │           3,550 │          18,574 │           3,550 │\n",
      "┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼\n",
      "│ Final           │          22,124 │          18,574 │           3,550 │\n",
      "└─────────────────┴─────────────────┴─────────────────┴─────────────────┘\n",
      "DataFrame after forward-filling missing dates:\n",
      "                   sentiment\n",
      "ticker date                 \n",
      "A      2024-08-06      0.051\n",
      "       2024-08-07      0.051\n",
      "       2024-08-08      0.131\n",
      "       2024-08-09      0.050\n",
      "       2024-08-10      0.050\n",
      "       2024-08-11      0.100\n",
      "       2024-08-12      0.042\n",
      "       2024-08-13      0.158\n",
      "       2024-08-14      0.158\n",
      "       2024-08-15      0.051\n",
      "       2024-08-16      0.051\n",
      "       2024-08-17      0.013\n",
      "       2024-08-18      0.013\n",
      "       2024-08-19      0.011\n",
      "       2024-08-20      0.011\n"
     ]
    }
   ],
   "source": [
    "df_sentiment = sov.data(\"news/sentiment\",full_history=True)\n",
    "\n",
    "df_sentiment = df_sentiment.filter([\"market_cap>50\"])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ================================\n",
    "# Step 1: Data Preparation\n",
    "# ================================\n",
    "\n",
    "# Reset index to convert 'ticker' and 'date' from MultiIndex to columns\n",
    "df_sentiment = df_sentiment.reset_index()\n",
    "\n",
    "# Ensure 'date' is in datetime format\n",
    "df_sentiment['date'] = pd.to_datetime(df_sentiment['date'])\n",
    "\n",
    "# Step 1: Find the global maximum date\n",
    "max_date = df_sentiment['date'].max()\n",
    "\n",
    "# Calculate the start date (12 weeks before the max_date)\n",
    "start_date = max_date - pd.Timedelta(weeks=12)\n",
    "\n",
    "# Keep only rows where 'date' is within the last 12 weeks from the maximum date\n",
    "df_sentiment = df_sentiment[df_sentiment['date'] >= start_date]\n",
    "\n",
    "# Sort the DataFrame by 'ticker' and 'date' to maintain chronological order\n",
    "df_sentiment = df_sentiment.sort_values(['ticker', 'date']).copy()\n",
    "\n",
    "\n",
    "# Reset index to convert 'ticker' and 'date' from MultiIndex to columns\n",
    "df_sentiment_reset = df_sentiment.reset_index()\n",
    "\n",
    "# Drop 'level_0' column if it exists\n",
    "if 'level_0' in df_sentiment_reset.columns:\n",
    "    df_sentiment_reset = df_sentiment_reset.drop(columns=['level_0'])\n",
    "\n",
    "# Ensure 'date' is in datetime format\n",
    "df_sentiment_reset['date'] = pd.to_datetime(df_sentiment_reset['date'])\n",
    "\n",
    "# Pivot the DataFrame: rows -> 'ticker', columns -> 'date', values -> 'sentiment'\n",
    "df_pivot = df_sentiment_reset.pivot(index='ticker', columns='date', values='sentiment')\n",
    "\n",
    "# Create a complete date range from the minimum to the maximum date\n",
    "complete_date_range = pd.date_range(start=df_pivot.columns.min(), end=df_pivot.columns.max(), freq='D')\n",
    "\n",
    "# Reindex the pivoted DataFrame to include all dates\n",
    "df_pivot = df_pivot.reindex(columns=complete_date_range)\n",
    "\n",
    "# Forward-fill missing sentiment values along the date axis\n",
    "df_pivot = df_pivot.ffill(axis=1)\n",
    "\n",
    "# Stack the DataFrame back to long format\n",
    "df_filled = df_pivot.stack().reset_index()\n",
    "\n",
    "# Rename the columns appropriately\n",
    "df_filled.columns = ['ticker', 'date', 'sentiment']\n",
    "\n",
    "# Set the MultiIndex back to ['ticker', 'date'] if needed\n",
    "df_filled = df_filled.set_index(['ticker', 'date'])\n",
    "\n",
    "# Display the first few rows of the filled DataFrame\n",
    "print(\"DataFrame after forward-filling missing dates:\")\n",
    "print(df_filled.head(15))\n",
    "\n",
    "\n",
    "# Calculate 6-day rolling average by ticker\n",
    "df_filled[\"sentiment\"] = df_filled.groupby('ticker')['sentiment'].rolling(window=6).mean().values\n",
    "\n",
    "\n",
    "df_filled = df_filled.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58e62632-5d15-4028-82e0-597623a91eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">AAPL</th>\n",
       "      <th>2024-08-11</th>\n",
       "      <td>-0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-12</th>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-13</th>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-14</th>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-15</th>\n",
       "      <td>0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-25</th>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-26</th>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-27</th>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-28</th>\n",
       "      <td>-0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-29</th>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sentiment\n",
       "ticker date                 \n",
       "AAPL   2024-08-11     -0.008\n",
       "       2024-08-12      0.010\n",
       "       2024-08-13      0.035\n",
       "       2024-08-14      0.049\n",
       "       2024-08-15      0.056\n",
       "...                      ...\n",
       "       2024-10-25      0.011\n",
       "       2024-10-26      0.019\n",
       "       2024-10-27      0.005\n",
       "       2024-10-28     -0.003\n",
       "       2024-10-29      0.004\n",
       "\n",
       "[80 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filled.query(\"ticker == 'AAPL'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1f952e5-3a31-4c59-b7b9-79831b0a5fc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of ['ticker'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tj/2qbc2n2x1234_l7b3z5y06740000gn/T/ipykernel_19534/853526114.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Step 3: Weekly Resampling and Metrics Calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# ================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Set the MultiIndex back for resampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mdf_sentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_sentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ticker\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Step 2: Get the weekday abbreviation (e.g., 'TUE' for Tuesday) from the max_date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mweekday_abbr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_date\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Sovai/GitHub/SovAI/.venv/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   6118\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6119\u001b[0m                         \u001b[0mmissing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of {missing} are in the columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6125\u001b[0m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of ['ticker'] are in the columns\""
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================\n",
    "# Step 2: Daily Metrics Calculation\n",
    "# ================================\n",
    "\n",
    "# Sort the DataFrame by 'ticker' and 'date' to maintain chronological order\n",
    "df_daily_sentiment = df_filled.reset_index().sort_values(['ticker', 'date']).copy()\n",
    "\n",
    "# Calculate 'last' sentiment by shifting 'sentiment' by one day within each 'ticker'\n",
    "df_daily_sentiment['last'] = df_daily_sentiment.groupby('ticker')['sentiment'].shift(1)\n",
    "\n",
    "# Calculate 'day change' as the difference between current and previous sentiment\n",
    "df_daily_sentiment['day change'] = df_daily_sentiment['sentiment'] - df_daily_sentiment['last']\n",
    "\n",
    "# Calculate 'long' as the difference between current 'sentiment' and the average sentiment over the last 12 weeks per 'ticker'\n",
    "df_daily_sentiment['long'] = df_daily_sentiment['sentiment'] - df_daily_sentiment.groupby('ticker')['sentiment'].transform('mean')\n",
    "\n",
    "# Drop rows with NaN values (e.g., first day per ticker where 'last' is NaN)\n",
    "df_daily_sentiment = df_daily_sentiment.dropna()\n",
    "\n",
    "# Filter to include only rows where 'date' == 'max_date' per ticker\n",
    "df_daily_sentiment = df_daily_sentiment[df_daily_sentiment[\"date\"].max() == df_daily_sentiment['date']]\n",
    "\n",
    "# Rename 'sentiment' to 'day_sentiment' for clarity\n",
    "df_daily_sentiment = df_daily_sentiment.rename(columns={\"sentiment\": \"day_sentiment\"})\n",
    "\n",
    "# Select relevant columns\n",
    "df_daily_sentiment = df_daily_sentiment[['ticker', 'date', 'day_sentiment', 'day change', 'long']]\n",
    "\n",
    "# ================================\n",
    "# Step 3: Weekly Resampling and Metrics Calculation\n",
    "# ================================\n",
    "\n",
    "# Set the MultiIndex back for resampling\n",
    "df_sentiment = df_sentiment.set_index([\"ticker\", \"date\"])\n",
    "\n",
    "# Step 2: Get the weekday abbreviation (e.g., 'TUE' for Tuesday) from the max_date\n",
    "weekday_abbr = max_date.strftime('%a').upper()[:3]\n",
    "\n",
    "# Define the resampling frequency to end on the max_date's weekday\n",
    "resample_freq = f'W-{weekday_abbr}'\n",
    "\n",
    "# Resample sentiment data weekly, taking the mean sentiment per week\n",
    "df_sentiment_resampled = (\n",
    "    df_sentiment\n",
    "    .groupby('ticker')\n",
    "    .resample(resample_freq, level='date')['sentiment']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Sort the resampled DataFrame by 'ticker' and 'date'\n",
    "df_sentiment_resampled = df_sentiment_resampled.sort_values(['ticker', 'date']).copy()\n",
    "\n",
    "# Create 'last' sentiment by shifting 'sentiment' by one week within each 'ticker'\n",
    "df_sentiment_resampled['last'] = df_sentiment_resampled.groupby('ticker')['sentiment'].shift(1)\n",
    "\n",
    "# Calculate 'week change' as the difference between current and previous sentiment\n",
    "df_sentiment_resampled['week change'] = df_sentiment_resampled['sentiment'] - df_sentiment_resampled['last']\n",
    "\n",
    "# Calculate 'long_term_change' as the difference between current 'sentiment' and the average sentiment over the last 12 weeks per 'ticker'\n",
    "df_sentiment_resampled['long'] = df_sentiment_resampled['sentiment'] - df_sentiment_resampled.groupby('ticker')['sentiment'].transform('mean')\n",
    "\n",
    "# Drop rows with NaN values (e.g., first week per ticker where 'last' is NaN)\n",
    "df_sentiment_resampled = df_sentiment_resampled.dropna()\n",
    "\n",
    "# Filter to include only rows where 'date' == 'max_date' per ticker\n",
    "df_sentiment_resampled = df_sentiment_resampled[df_sentiment_resampled[\"date\"].max() == df_sentiment_resampled['date']]\n",
    "\n",
    "# Rename 'sentiment' to 'week_sentiment' for clarity\n",
    "df_sentiment_resampled = df_sentiment_resampled.rename(columns={\"sentiment\": \"week_sentiment\"})\n",
    "\n",
    "# Select relevant columns\n",
    "df_sentiment_resampled = df_sentiment_resampled[['ticker', 'date', 'week_sentiment','last', 'week change', 'long']]\n",
    "\n",
    "# ================================\n",
    "# Step 4: Merging Daily and Weekly Metrics\n",
    "# ================================\n",
    "\n",
    "# Merge the daily and weekly sentiment DataFrames on ['ticker', 'date']\n",
    "df_final_sentiment = pd.merge(\n",
    "    df_sentiment_resampled,\n",
    "    df_daily_sentiment,\n",
    "    on=['ticker', 'date'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# Step 5: Final Cleanup and Column Ordering\n",
    "# ================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "72ec1a9e-6f98-4265-b8b8-9eb614b2b73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The following columns are missing and will be excluded from the final DataFrame: ['short']\n",
      "    ticker  sentiment  last  week change   long  day change\n",
      "53    ALCO      0.029 0.550       -0.521 -0.108      -0.024\n",
      "242   CCOI     -0.400 0.100       -0.500 -0.315       0.000\n",
      "998   PLCE     -0.200 0.300       -0.500  0.056      -0.083\n",
      "656   INCY      0.010 0.500       -0.490  0.101      -0.083\n",
      "948   OSPN     -0.356 0.100       -0.456 -0.100      -0.100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>last</th>\n",
       "      <th>week change</th>\n",
       "      <th>day change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALCO</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.550</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>-0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCOI</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PLCE</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.300</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INCY</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.500</td>\n",
       "      <td>-0.490</td>\n",
       "      <td>-0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OSPN</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.456</td>\n",
       "      <td>-0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>GLAD</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>CNQ</td>\n",
       "      <td>0.400</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>WGO</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>SNAP</td>\n",
       "      <td>0.400</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>TFII</td>\n",
       "      <td>0.500</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1420 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ticker  sentiment   last  week change  day change\n",
       "0      ALCO      0.029  0.550       -0.521      -0.024\n",
       "1      CCOI     -0.400  0.100       -0.500       0.000\n",
       "2      PLCE     -0.200  0.300       -0.500      -0.083\n",
       "3      INCY      0.010  0.500       -0.490      -0.083\n",
       "4      OSPN     -0.356  0.100       -0.456      -0.100\n",
       "...     ...        ...    ...          ...         ...\n",
       "1415   GLAD      0.100 -0.350        0.450       0.075\n",
       "1416    CNQ      0.400 -0.062        0.463       0.077\n",
       "1417    WGO      0.080 -0.400        0.480       0.060\n",
       "1418   SNAP      0.400 -0.100        0.500       0.083\n",
       "1419   TFII      0.500 -0.062        0.562       0.094\n",
       "\n",
       "[1420 rows x 5 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename columns to match the desired 'new_order'\n",
    "df_final_sentiment = df_final_sentiment.rename(columns={\n",
    "    'week_sentiment': 'sentiment',       # Current sentiment from weekly data\n",
    "    'last': 'last',                    # Last week's sentiment\n",
    "    'change': 'week change',      # Weekly change\n",
    "    'long_x': 'long',                    # Long-term change from weekly data\n",
    "    'day change': 'day change',          # Daily change\n",
    "    'long_y': 'short'                    # Short-term change from daily data\n",
    "})\n",
    "\n",
    "# Select and reorder the columns\n",
    "new_order = ['ticker', 'sentiment', 'last', 'week change', 'long', 'day change', 'short']\n",
    "\n",
    "# Ensure all columns in new_order exist in df_final_sentiment\n",
    "existing_columns = [col for col in new_order if col in df_final_sentiment.columns]\n",
    "missing_columns = [col for col in new_order if col not in df_final_sentiment.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"Warning: The following columns are missing and will be excluded from the final DataFrame: {missing_columns}\")\n",
    "\n",
    "# Reorder the DataFrame\n",
    "df_final_sentiment = df_final_sentiment[existing_columns]\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(df_final_sentiment.head())\n",
    "\n",
    "df_final_sentiment = df_final_sentiment.drop(columns=[\"long\"]).rename(columns={\"short\":\"long\"})\n",
    "\n",
    "df_final_sentiment\n",
    "\n",
    "df_final_sentiment = df_final_sentiment.reset_index(drop=True)\n",
    "\n",
    "df_final_sentiment = df_final_sentiment.sort_values(\"week change\")\n",
    "\n",
    "df_final_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c808ae50-6156-4790-9530-48faa4e9d8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pressure columns kept and their order: ['sent_0', 'sent_1', 'sent_5', 'sent_9', 'sent_13', 'sent_17', 'sent_21', 'sent_25', 'sent_29', 'sent_33', 'sent_37', 'sent_41', 'sent_45', 'sent_49', 'sent_53', 'sent_57', 'sent_60']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['long'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 75\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPressure columns kept and their order:\u001b[39m\u001b[38;5;124m\"\u001b[39m, pressure_columns)\n\u001b[1;32m     73\u001b[0m df_sentiment_expanded\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mticker == \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAAPL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtail(\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m df_sentiment_expanded[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweek change\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday change\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_sentiment_expanded\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentiment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlast\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweek change\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlong\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mday change\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     80\u001b[0m columns_to_process \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweek change\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday change\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Sovai/GitHub/SovAI/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/Sovai/GitHub/SovAI/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/Sovai/GitHub/SovAI/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['long'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_pressure_history_columns(df_org, df_wiki, lookback=60):\n",
    "    \"\"\"\n",
    "    Creates pressure history columns from df_org and merges them with df_wiki.\n",
    "\n",
    "    Parameters:\n",
    "    - df_org (pd.DataFrame): Original DataFrame with a MultiIndex including 'date'.\n",
    "    - df_wiki (pd.DataFrame): DataFrame to merge the pressure columns into.\n",
    "    - lookback (int): Number of days to look back for pressure data.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Merged DataFrame with pressure history columns.\n",
    "    \"\"\"\n",
    "    # 1. Get the maximum date from index level 'date'\n",
    "    max_date = df_org.index.get_level_values('date').max()\n",
    "    \n",
    "    # 2. Calculate the cutoff date\n",
    "    cutoff_date = max_date - pd.Timedelta(days=lookback)\n",
    "    \n",
    "    # 3. Filter df_org to only include data after cutoff date\n",
    "    df_filtered = df_org[df_org.index.get_level_values('date') >= cutoff_date]\n",
    "    \n",
    "    # 4. Create pivot table with filtered data\n",
    "    df_pivot = df_filtered.reset_index().pivot(\n",
    "        index='ticker', \n",
    "        columns='date', \n",
    "        values='sentiment'\n",
    "    )\n",
    "    \n",
    "    # 5. Sort the columns by date ascendingly to ensure pressure_0 is the earliest\n",
    "    df_pivot = df_pivot.sort_index(axis=1)\n",
    "    \n",
    "    # 6. Create pressure column names in ascending order\n",
    "    num_cols = len(df_pivot.columns)\n",
    "    all_pressure_cols = [f'sent_{i}' for i in range(num_cols)]\n",
    "    df_pivot.columns = all_pressure_cols\n",
    "    \n",
    "    # 7. Select columns: first, every 4th, and last\n",
    "    pressure_cols_to_keep = [all_pressure_cols[0]]  # First column (pressure_0)\n",
    "    if len(all_pressure_cols) > 2:  # If we have middle columns\n",
    "        pressure_cols_to_keep.extend(all_pressure_cols[1:-1:4])  # Every 4th column\n",
    "    pressure_cols_to_keep.append(all_pressure_cols[-1])  # Last column (pressure_{num_cols-1})\n",
    "    \n",
    "    # 8. Keep only selected columns\n",
    "    df_pivot = df_pivot[pressure_cols_to_keep]\n",
    "    \n",
    "    # 9. Reset index to make ticker a column\n",
    "    df_pivot = df_pivot.reset_index()\n",
    "    \n",
    "    # 10. Merge with original df_wiki\n",
    "    df_wiki_expanded = df_wiki.merge(df_pivot, on='ticker', how='left')\n",
    "    \n",
    "    return df_wiki_expanded\n",
    "\n",
    "# Example Usage:\n",
    "\n",
    "# Assuming you have df_org and df_wiki already defined\n",
    "# df_org should have a MultiIndex with 'date' and 'ticker'\n",
    "# df_wiki is the DataFrame you want to expand with pressure columns\n",
    "\n",
    "# Apply the function\n",
    "\n",
    "\n",
    "df_sentiment_expanded = create_pressure_history_columns(df_filled, df_final_sentiment)\n",
    "\n",
    "# To check the columns we kept and their order\n",
    "pressure_columns = [col for col in df_sentiment_expanded.columns if col.startswith('sent_')]\n",
    "print(\"Pressure columns kept and their order:\", pressure_columns)\n",
    "\n",
    "\n",
    "df_sentiment_expanded.query(\"ticker == 'AAPL'\").tail(20)\n",
    "\n",
    "df_sentiment_expanded[[\"sentiment\",\"last\",\"week change\",\"long\",\"day change\"]] = df_sentiment_expanded[[\"sentiment\",\"last\",\"week change\",\"long\",\"day change\"]]*100\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "columns_to_process = ['sentiment', 'week change', 'long', 'day change']\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "top_bottom_dfs = []\n",
    "\n",
    "# Iterate over each column\n",
    "for col in columns_to_process:\n",
    "    # Ensure the column exists in the DataFrame\n",
    "    if col not in df_sentiment_expanded.columns:\n",
    "        print(f\"Column '{col}' does not exist in the DataFrame.\")\n",
    "        continue\n",
    "\n",
    "    # Sort ascending to get bottom 15\n",
    "    bottom_15 = df_sentiment_expanded.sort_values(by=col, ascending=True).head(50).copy()\n",
    "    \n",
    "    # Sort descending to get top 15\n",
    "    top_15 = df_sentiment_expanded.sort_values(by=col, ascending=False).head(50).copy()\n",
    "    \n",
    "    # Append to the list\n",
    "    top_bottom_dfs.extend([top_15, bottom_15])\n",
    "\n",
    "# Concatenate all DataFrames in the list\n",
    "combined_df = pd.concat(top_bottom_dfs, ignore_index=True)\n",
    "\n",
    "# Remove duplicate rows\n",
    "combined_df_unique = combined_df.drop_duplicates()\n",
    "\n",
    "# Reset index for cleanliness\n",
    "combined_df_unique.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "combined_df_unique = combined_df_unique.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70dc46f4-abda-4797-92fe-ff7f61d9c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import locale\n",
    "\n",
    "# Set locale to US English\n",
    "locale.setlocale(locale.LC_TIME, 'en_US.UTF-8')\n",
    "\n",
    "\n",
    "def get_week_ending_label(reference_date=None):\n",
    "    \"\"\"\n",
    "    Returns a formatted string indicating the week ending on the last Friday relative to the reference date.\n",
    "\n",
    "    Args:\n",
    "        reference_date (datetime.date, optional): The date to reference. Defaults to today.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted string like \"Week ending Friday 25th October, 2024\"\n",
    "    \"\"\"\n",
    "    if reference_date is None:\n",
    "        reference_date = datetime.date.today()\n",
    "    \n",
    "    def get_ordinal(n):\n",
    "        if 11 <= n % 100 <= 13:\n",
    "            suffix = 'th'\n",
    "        else:\n",
    "            suffix = {1: 'st', 2: 'nd', 3: 'rd'}.get(n % 10, 'th')\n",
    "        return f\"{n}{suffix}\"\n",
    "    \n",
    "    days_since_friday = (reference_date.weekday() - 4) % 7\n",
    "    last_friday = reference_date - datetime.timedelta(days=days_since_friday)\n",
    "    day_with_ordinal = get_ordinal(last_friday.day)\n",
    "    formatted_date = f\"Week ending {last_friday.strftime('%A')} {day_with_ordinal} {last_friday.strftime('%B')}, {last_friday.year}\"\n",
    "    \n",
    "    return formatted_date\n",
    "\n",
    "# Usage\n",
    "formatted_week_label = get_week_ending_label()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "430e04e7-434b-407d-b047-c5572980fb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published Chart URL: [{'id': 'standalone', 'url': 'https://www.datawrapper.de/_/4yxI4/', 'name': 'For sharing'}]\n"
     ]
    }
   ],
   "source": [
    "from datawrapper import Datawrapper\n",
    "\n",
    "# Initialize Datawrapper\n",
    "dw = Datawrapper(access_token=\"your_token\")\n",
    "\n",
    "# Create the chart\n",
    "chart = dw.create_chart(\n",
    "    title=\"Stock Sentiment Analysis\",\n",
    "    chart_type=\"tables\"\n",
    ")\n",
    "\n",
    "# Add the data to the chart\n",
    "dw.add_data(chart['id'], data=combined_df_unique)\n",
    "\n",
    "# Get pressure column names\n",
    "pressure_cols = [col for col in combined_df_unique.columns if col.startswith('sent_')]\n",
    "\n",
    "# Configure the visualization properties\n",
    "properties = {\n",
    "     \"visualize\": {\n",
    "        \"dark-mode-invert\": True,\n",
    "        \"perPage\": 20,\n",
    "        \"columns\": {\n",
    "            \"ticker\": {\n",
    "                \"align\": \"left\",\n",
    "                \"title\": \"Stock\",\n",
    "                \"width\": \"100\"\n",
    "            },\n",
    "            \"search\": {\n",
    "                \"title\": \"Sentiment\",\n",
    "                \"format\": \"0.000\",\n",
    "                \"width\": \"120\"\n",
    "            },\n",
    "            \"last\": {\n",
    "                \"title\": \"Last\",\n",
    "                \"format\": \"0.000\",\n",
    "                \"width\": \"120\"\n",
    "            },\n",
    "            \"week change\": {\n",
    "                \"title\": \"Change\",\n",
    "                \"format\": \"+0.000\",\n",
    "                \"width\": 0.27,  # Updated to match working example\n",
    "                \"showAsBar\": True,\n",
    "                \"barColorNegative\": \"#ff4444\",\n",
    "                \"fixedWidth\": True\n",
    "            },\n",
    "            \"long\": {\n",
    "                \"title\": \"Long Trend\",\n",
    "                \"format\": \"+0.000\",\n",
    "                \"width\": \"120\"\n",
    "            },\n",
    "            \"day change\": {\n",
    "                \"title\": \"Day Change\",\n",
    "                \"format\": \"+0.000\",\n",
    "                \"width\": 0.17,  # Updated to match working example\n",
    "                \"fixedWidth\": True\n",
    "            },\n",
    "            \"pressure_0\": {\n",
    "                \"type\": \"number\",\n",
    "                \"title\": \"History\",  # Updated to just \"History\"\n",
    "                \"width\": 0.33,  # Updated to match working example\n",
    "                \"format\": \"0.000\",\n",
    "                \"sparkline\": {\n",
    "                    \"color\": \"#18a1cd\",\n",
    "                    \"title\": \"History\",\n",
    "                    \"enabled\": True,\n",
    "                    \"stroke\": 2,\n",
    "                    \"dotMax\": True,\n",
    "                    \"dotMin\": True,\n",
    "                    \"dotFirst\": True,\n",
    "                    \"dotLast\": True\n",
    "                },\n",
    "                \"fixedWidth\": True\n",
    "            }\n",
    "        },\n",
    "        \"header\": {\n",
    "            \"style\": {\n",
    "                \"bold\": True,\n",
    "                \"fontSize\": 0.9,\n",
    "                \"color\": \"#494949\"\n",
    "            },\n",
    "            \"borderBottom\": \"2px\",\n",
    "            \"borderBottomColor\": \"#333333\"\n",
    "        },\n",
    "        \"pagination\": {\n",
    "            \"enabled\": True,\n",
    "            \"position\": \"bottom\",\n",
    "            \"pagesPerScreen\": 10\n",
    "        },\n",
    "        \"striped\": True,\n",
    "        \"markdown\": True,\n",
    "        \"showHeader\": True,\n",
    "        \"compactMode\": True,\n",
    "        \"firstRowIsHeader\": False,\n",
    "        \"firstColumnIsSticky\": False,\n",
    "        \"mergeEmptyCells\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "# Configure remaining pressure columns exactly like the first one\n",
    "for col in pressure_cols:\n",
    "    properties[\"visualize\"][\"columns\"][col] = {\n",
    "        \"type\": \"number\",\n",
    "        \"width\": 0.33,  # Updated to match working example\n",
    "        \"format\": \"0.000\",\n",
    "        \"sparkline\": {\n",
    "            \"color\": \"#18a1cd\",\n",
    "            \"title\": \"sentiment_history\",\n",
    "            \"enabled\": True\n",
    "        },\n",
    "        \"fixedWidth\": True\n",
    "    }\n",
    "\n",
    "# Set column order\n",
    "properties[\"visualize\"][\"column-order\"] = [\n",
    "    \"ticker\",\n",
    "    \"search\",\n",
    "    \"last\",\n",
    "    \"weel change\",\n",
    "    \"long\",\n",
    "    \"day change\",\n",
    "] + pressure_cols\n",
    "\n",
    "current_date = datetime.datetime.now().strftime(\"%B %d, %Y\")\n",
    "\n",
    "\n",
    "# Add other visualization settings\n",
    "properties[\"describe\"] = {\n",
    "    \"intro\": f\"Analysis of news pressure for companies with trends over 60 days, as of {current_date}. Derived from <a href='https://docs.sov.ai/realtime-datasets/equity-datasets/news-sentiment'>Sov.ai™ News</a> datasets.\",\n",
    "    \"byline\": \"\",\n",
    "    \"source-name\": \"Sentiment Data\",\n",
    "    \"source-url\": \"\",\n",
    "    \"hide-title\": False\n",
    "}\n",
    "\n",
    "properties[\"publish\"] = {\n",
    "    \"embed-width\": 682,\n",
    "    \"embed-height\": 1086,\n",
    "    \"blocks\": {\n",
    "        \"logo\": {\"enabled\": False},\n",
    "        \"embed\": False,\n",
    "        \"download-pdf\": False,\n",
    "        \"download-svg\": False,\n",
    "        \"get-the-data\": True,\n",
    "        \"download-image\": False\n",
    "    },\n",
    "    \"autoDarkMode\": False,\n",
    "    \"chart-height\": 988,\n",
    "    \"force-attribution\": False\n",
    "}\n",
    "\n",
    "# Update the chart with the properties\n",
    "dw.update_chart(\n",
    "    chart['id'],\n",
    "    metadata=properties\n",
    ")\n",
    "\n",
    "# Publish the chart\n",
    "dw.publish_chart(chart['id'])\n",
    "\n",
    "# Get the published URL\n",
    "published_url = dw.get_chart_display_urls(chart['id'])\n",
    "print(\"Published Chart URL:\", published_url)\n",
    "news_url = published_url[0][\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d87efc8-2869-48a2-bd82-f927c21ae4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_url = published_url[0][\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84a6e2c9-f8b5-468b-9140-494a5696d6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition 1: market_cap>50 (Standard filter)\n",
      "\n",
      "Filtering Results:\n",
      "┌─────────────────┬─────────────────┬─────────────────┬─────────────────┐\n",
      "│ Step            │   Total Tickers │         Removed │            Left │\n",
      "┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼\n",
      "│ Initial         │          22,124 │               - │               - │\n",
      "│ Condition 1     │           3,550 │          18,574 │           3,550 │\n",
      "┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼\n",
      "│ Final           │          22,124 │          18,574 │           3,550 │\n",
      "└─────────────────┴─────────────────┴─────────────────┴─────────────────┘\n"
     ]
    }
   ],
   "source": [
    "## Secrtorial Sentiment\n",
    "\n",
    "df_sentiment_sect = sov.data(\"news/sentiment\",full_history=True)\n",
    "\n",
    "df_sentiment_sect = df_sentiment_sect.filter([\"market_cap>50\"])\n",
    "\n",
    "df_sentiment_sect = df_sentiment_sect.reset_index()\n",
    "\n",
    "# Calculate the start date (12 weeks before the max_date)\n",
    "start_date = max_date - pd.Timedelta(weeks=12)\n",
    "\n",
    "# Ensure 'date' is in datetime format\n",
    "df_sentiment_sect['date'] = pd.to_datetime(df_sentiment_sect['date'])\n",
    "\n",
    "df_sentiment_sect = df_sentiment_sect.merge(tickers_meta[[\"ticker\",\"sector\"]], on=\"ticker\",how=\"left\")\n",
    "\n",
    "df_sentiment_sect = df_sentiment_sect.drop(columns=[\"ticker\"]).groupby([\"date\",\"sector\"]).mean().sort_index().reset_index()\n",
    "\n",
    "\n",
    "\n",
    "df_wide = df_sentiment_sect.pivot(index='date', columns='sector', values='sentiment')\n",
    "\n",
    "\n",
    "df_wide = df_wide.resample(\"D\").ffill()  \n",
    "\n",
    "df_wide = df_wide.rolling(20).mean()\n",
    " \n",
    "df_wide = df_wide.tail(180).dropna()\n",
    "\n",
    "df_ranks = df_wide.rank(pct=True, axis=0).reset_index()\n",
    "\n",
    "\n",
    "df_ranks = df_ranks.set_index(\"date\").rank(pct=True,axis=1)\n",
    "\n",
    "df_ranks = df_ranks.rolling(20).mean().dropna()\n",
    "\n",
    "df_ranks = df_ranks.rank(pct=True,axis=1)\n",
    "\n",
    "df_ranks = df_ranks.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4ccd92e-3d98-48ea-9e8e-e9e44c6e55ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 24\u001b[0m\n\u001b[1;32m      9\u001b[0m colors \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#0066CC\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Deep blue\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#1975D1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#FF8000\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Deep orange\u001b[39;00m\n\u001b[1;32m     21\u001b[0m ]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Create color mapping based on final positions\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m color_mapping \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43mseries\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseries\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_positions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Initialize Datawrapper\u001b[39;00m\n\u001b[1;32m     27\u001b[0m dw \u001b[38;5;241m=\u001b[39m Datawrapper(access_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZ9FqsGcQfCaeERekoelA1IjU4RBo4XcPhfsGY935YVNol54uN3QzITdFSZphaQov\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[41], line 24\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m colors \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#0066CC\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Deep blue\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#1975D1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#FF8000\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Deep orange\u001b[39;00m\n\u001b[1;32m     21\u001b[0m ]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Create color mapping based on final positions\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m color_mapping \u001b[38;5;241m=\u001b[39m {series: \u001b[43mcolors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i, series \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(final_positions)}\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Initialize Datawrapper\u001b[39;00m\n\u001b[1;32m     27\u001b[0m dw \u001b[38;5;241m=\u001b[39m Datawrapper(access_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZ9FqsGcQfCaeERekoelA1IjU4RBo4XcPhfsGY935YVNol54uN3QzITdFSZphaQov\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from datawrapper import Datawrapper\n",
    "\n",
    "# First get the order of series from the last row of your dataframe\n",
    "last_row = df_ranks.drop(columns=[\"date\"]).iloc[-1]  # Get the last row\n",
    "# Sort the columns by their values in descending order\n",
    "final_positions = last_row.sort_values(ascending=False).index.tolist()\n",
    "\n",
    "# Create color gradient\n",
    "colors = [\n",
    "    \"#0066CC\",  # Deep blue\n",
    "    \"#1975D1\",\n",
    "    \"#4D94DB\",\n",
    "    \"#66A3E0\",\n",
    "    \"#80B2E6\",\n",
    "    \"#CCE0F3\",\n",
    "    \"#FFE5CC\",\n",
    "    \"#FFCC99\",\n",
    "    \"#FFB366\",\n",
    "    \"#FF9933\",\n",
    "    \"#FF8000\",  # Deep orange\n",
    "]\n",
    "\n",
    "# Create color mapping based on final positions\n",
    "color_mapping = {series: colors[i] for i, series in enumerate(final_positions)}\n",
    "\n",
    "# Initialize Datawrapper\n",
    "dw = Datawrapper(access_token=\"your_token\")\n",
    "# Create a new line chart\n",
    "chart = dw.create_chart(\n",
    "    title=\"Daily Sectorial Sentiment Rank\",\n",
    "    chart_type=\"d3-lines\"\n",
    ")\n",
    "\n",
    "# Configure the visualization properties\n",
    "metadata = {\n",
    "    \"visualize\": {\n",
    "        \"dark-mode-invert\": True,\n",
    "        \"interpolation\": \"natural\",\n",
    "        \"x-grid\": \"ticks\",\n",
    "        \"y-grid\": \"off\",\n",
    "        \"opacity\": 1,\n",
    "        \"scale-y\": \"linear\",\n",
    "        \"base-color\": 8,\n",
    "        \"line-width\": 2,\n",
    "        \"label-colors\": True,\n",
    "        \"label-margin\": 138,\n",
    "        \"stack-to-100\": False,\n",
    "        \"show-tooltips\": True,\n",
    "        \"x-grid-format\": \"YYYY-MM-DD\",\n",
    "        \"y-grid-format\": \"auto\",\n",
    "        \"y-grid-labels\": \"auto\",\n",
    "        \"plotHeightMode\": \"ratio\",\n",
    "        \"plotHeightRatio\": 0.62,\n",
    "        \"plotHeightFixed\": 300,\n",
    "        \"color-by-column\": True,\n",
    "        \"connector-lines\": True,\n",
    "        \"y-grid-subdivide\": True,\n",
    "        \"value-label-colors\": True,\n",
    "        \"y-grid-label-align\": \"left\",\n",
    "        \"tooltip\": {\n",
    "            \"sticky\": True,\n",
    "            \"enabled\": True\n",
    "        },\n",
    "        \"lines\": {\n",
    "            series: {\n",
    "                \"color\": color_mapping[series],\n",
    "                \"symbols\": {\"size\": 3, \"style\": \"hollow\", \"enabled\": True}\n",
    "            } for series in final_positions\n",
    "        }\n",
    "    },\n",
    "    \"describe\": {\n",
    "        \"byline\": \"Scraping various news related datasets\",\n",
    "        \"intro\": \"The data has been collected since 2016 and allows us to identify daily changes in news sentiment accross sectors.\",\n",
    "        \"source-name\": \"\",\n",
    "        \"source-url\": \"\"\n",
    "    },\n",
    "    \"axes\": {\n",
    "        \"x\": \"date\"\n",
    "    },\n",
    "    \"publish\": {\n",
    "        \"embed-width\": 628,\n",
    "        \"embed-height\": 411,\n",
    "        \"blocks\": {\n",
    "            \"logo\": {\"enabled\": False},\n",
    "            \"embed\": False,\n",
    "            \"download-pdf\": False,\n",
    "            \"download-svg\": False,\n",
    "            \"get-the-data\": True,\n",
    "            \"download-image\": False\n",
    "        },\n",
    "        \"chart-height\": 307\n",
    "    }\n",
    "}\n",
    "\n",
    "# Update the chart with our configuration\n",
    "dw.update_chart(chart['id'], metadata=metadata)\n",
    "# Add the data to the chart\n",
    "dw.add_data(chart['id'], data=df_ranks)\n",
    "# Publish the chart\n",
    "dw.publish_chart(chart['id'])\n",
    "# Get the embed code\n",
    "\n",
    "# Get the published URL\n",
    "published_url = dw.get_chart_display_urls(chart['id'])\n",
    "print(\"Published Chart URL:\", published_url)\n",
    "sector_url = published_url[0][\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa6285be-8e60-4f50-85a5-d178eed69fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>last</th>\n",
       "      <th>week change</th>\n",
       "      <th>day change</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>ALCO</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.550</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>CCOI</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>PLCE</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.300</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>INCY</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.500</td>\n",
       "      <td>-0.490</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>OSPN</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.456</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ticker  sentiment  last  week change  day change   long\n",
       "53    ALCO      0.029 0.550       -0.521      -0.024 -0.108\n",
       "242   CCOI     -0.400 0.100       -0.500       0.000 -0.315\n",
       "998   PLCE     -0.200 0.300       -0.500      -0.083  0.056\n",
       "656   INCY      0.010 0.500       -0.490      -0.083  0.101\n",
       "948   OSPN     -0.356 0.100       -0.456      -0.100 -0.100"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c28d3a12-ad3d-4d40-8d7c-81b489d02573",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = sov.data(\"news/sentiment_score\", full_history=True)\n",
    "\n",
    "df_smaller = df_sentiment[df_sentiment[\"calculation\"]==\"sentiment_score_median\"].reset_index(drop=True).drop(columns=[\"calculation\"]).set_index(\"date\").rolling(60).mean().dropna().tail(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "179c5fc6-dcf3-42ca-a52e-81ed25e52945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Column                 Theme\n",
      "0                monetary_policy       Monetary Policy\n",
      "1                 interest_rates       Monetary Policy\n",
      "2                     tax_policy         Fiscal Policy\n",
      "3                economic_growth     Growth Indicators\n",
      "4                      inflation             Inflation\n",
      "5            international_trade  Intl Trade & Finance\n",
      "6               energy_resources               Sectors\n",
      "7        artificial_intelligence            Technology\n",
      "8    financial_markets_investing     Financial Markets\n",
      "9   environmental_sustainability                   ESG\n",
      "10      demographic_shifts_aging         Social Trends\n",
      "11           geopolitical_events           Geopolitics\n",
      "12                infrastructure        Infrastructure\n",
      "13                  labor_market          Labor Market\n",
      "14  consulting_business_services           Regulations\n",
      "15     insurance_risk_management       Risk Management\n",
      "Warning: The following columns for theme 'Fiscal Policy' are missing in df_smaller: ['tax_policy', 'spending_policy']\n",
      "Warning: The following columns for theme 'Inflation' are missing in df_smaller: ['price_stability', 'cost_of_living']\n",
      "Warning: The following columns for theme 'Technology' are missing in df_smaller: ['quantum_computing']\n",
      "Warning: The following columns for theme 'ESG' are missing in df_smaller: ['social_responsibility', 'governance_standards']\n",
      "            Monetary Policy  Fiscal Policy  Growth Indicators  Inflation  \\\n",
      "date                                                                       \n",
      "2024-08-01            0.153          0.186              0.164      0.167   \n",
      "2024-08-02            0.149          0.181              0.160      0.163   \n",
      "2024-08-03            0.141          0.172              0.152      0.154   \n",
      "2024-08-04            0.140          0.171              0.151      0.153   \n",
      "2024-08-05            0.139          0.169              0.150      0.151   \n",
      "\n",
      "            Intl Trade & Finance  Sectors  Technology  Financial Markets  \\\n",
      "date                                                                       \n",
      "2024-08-01                 0.158    0.159       0.158              0.180   \n",
      "2024-08-02                 0.154    0.155       0.154              0.175   \n",
      "2024-08-03                 0.147    0.147       0.147              0.166   \n",
      "2024-08-04                 0.146    0.146       0.145              0.165   \n",
      "2024-08-05                 0.144    0.145       0.144              0.163   \n",
      "\n",
      "             ESG  Social Trends  Geopolitics  Infrastructure  Labor Market  \\\n",
      "date                                                                         \n",
      "2024-08-01 0.162          0.177        0.166           0.135         0.155   \n",
      "2024-08-02 0.158          0.172        0.161           0.131         0.151   \n",
      "2024-08-03 0.149          0.164        0.153           0.125         0.143   \n",
      "2024-08-04 0.148          0.163        0.152           0.124         0.143   \n",
      "2024-08-05 0.147          0.161        0.150           0.123         0.141   \n",
      "\n",
      "            Regulations  Risk Management  \n",
      "date                                      \n",
      "2024-08-01        0.172            0.177  \n",
      "2024-08-02        0.167            0.172  \n",
      "2024-08-03        0.159            0.164  \n",
      "2024-08-04        0.158            0.163  \n",
      "2024-08-05        0.156            0.161  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Utilities',\n",
       " 'Consumer Defensive',\n",
       " 'Consumer Cyclical',\n",
       " 'Technology',\n",
       " 'Financial Services',\n",
       " 'Communication Services',\n",
       " 'Energy',\n",
       " 'Real Estate',\n",
       " 'Industrials',\n",
       " 'Healthcare',\n",
       " 'Basic Materials']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Revised mapping dictionary with 15 succinct themes\n",
    "theme_mapping = {\n",
    "    'Monetary Policy': [\n",
    "        'monetary_policy',\n",
    "        'monetary_policy_transmission',\n",
    "        'interest_rates',\n",
    "        'quantitative_easing',\n",
    "        'central_bank_digital_currencies'\n",
    "    ],\n",
    "    'Fiscal Policy': [\n",
    "        'fiscal_policy',\n",
    "        'government_debt_deficit',\n",
    "        'tax_policy',\n",
    "        'spending_policy'\n",
    "    ],\n",
    "    'Growth Indicators': [\n",
    "        'economic_growth',\n",
    "        'business_cycles',\n",
    "        'economic_forecasting_modeling'\n",
    "    ],\n",
    "    'Inflation': [\n",
    "        'inflation',\n",
    "        'price_stability',\n",
    "        'cost_of_living'\n",
    "    ],\n",
    "    'Intl Trade & Finance': [\n",
    "        'international_trade',\n",
    "        'foreign_direct_investment',\n",
    "        'foreign_exchange_markets',\n",
    "        'trade_agreements',\n",
    "        'international_monetary_system',\n",
    "        'international_finance',\n",
    "        'emerging_economies',\n",
    "        'sanctions_embargoes',\n",
    "        'international_development_aid'\n",
    "    ],\n",
    "    'Sectors': [\n",
    "        'energy_resources',\n",
    "        'healthcare_pharma',\n",
    "        'real_estate_housing',\n",
    "        'consumer_spending_retail',\n",
    "        'manufacturing_industrial',\n",
    "        'transportation_logistics',\n",
    "        'agriculture_food',\n",
    "        'aerospace_defense',\n",
    "        'utilities_public_services',\n",
    "        'mining_extraction',\n",
    "        'chemicals_materials',\n",
    "        'forestry_paper_products',\n",
    "        'fishing_aquaculture',\n",
    "        'textiles_apparel',\n",
    "        'luxury_goods_services',\n",
    "        'sports_entertainment',\n",
    "        'media_publishing'\n",
    "    ],\n",
    "    'Technology': [\n",
    "        'artificial_intelligence',\n",
    "        'robotics_automation',\n",
    "        'cybersecurity_data_privacy',\n",
    "        'cryptocurrency_blockchain',\n",
    "        'quantum_computing',\n",
    "        'technology_innovation',\n",
    "        'intellectual_property_patents',\n",
    "        'nanotech_advanced_materials',\n",
    "        'space_commercialization_exploration',\n",
    "        'renewable_energy',\n",
    "        'digital_economy',\n",
    "        'financial_technology_fintech'\n",
    "    ],\n",
    "    'Financial Markets': [\n",
    "        'financial_markets_investing',\n",
    "        'insurance_risk_management',\n",
    "        'private_equity_venture_capital',\n",
    "        'sovereign_wealth_funds',\n",
    "        'pension_funds',\n",
    "        'hedge_funds',\n",
    "        'exchange_traded_funds',\n",
    "        'mergers_acquisitions',\n",
    "        'initial_public_offerings',\n",
    "        'bond_markets',\n",
    "        'derivative_markets',\n",
    "        'yield_curve',\n",
    "        'credit_ratings',\n",
    "        'high_frequency_trading',\n",
    "        'algorithmic_trading',\n",
    "        'robo_advisors'\n",
    "    ],\n",
    "    'ESG': [\n",
    "        'environmental_sustainability',\n",
    "        'climate_change',\n",
    "        'renewable_energy',\n",
    "        'circular_economy',\n",
    "        'sustainable_development',\n",
    "        'human_rights_business',\n",
    "        'social_responsibility',\n",
    "        'governance_standards'\n",
    "    ],\n",
    "    'Social Trends': [\n",
    "        'demographic_shifts_aging',\n",
    "        'poverty_alleviation',\n",
    "        'education_human_capital',\n",
    "        'income_inequality',\n",
    "        'welfare_inequality',\n",
    "        'labor_market',\n",
    "        'labor_productivity',\n",
    "        'education_services',\n",
    "        'sharing_economy_gig_work'\n",
    "    ],\n",
    "    'Geopolitics': [\n",
    "        'geopolitical_events',\n",
    "        'sanctions_embargoes',\n",
    "        'migration_remittances',\n",
    "        'international_development_aid',\n",
    "        'foreign_direct_investment',\n",
    "        'foreign_exchange_markets'\n",
    "    ],\n",
    "    'Infrastructure': [\n",
    "        'urbanization_city_planning',\n",
    "        'infrastructure',\n",
    "        'transportation_logistics',\n",
    "        'utilities_public_services',\n",
    "        'waste_management_recycling'\n",
    "    ],\n",
    "    'Labor Market': [\n",
    "        'labor_market',\n",
    "        'labor_productivity',\n",
    "        'small_business_entrepreneurship',\n",
    "        'consulting_business_services',\n",
    "        'legal_services_regulations'\n",
    "    ],\n",
    "    'Regulations': [\n",
    "        'consulting_business_services',\n",
    "        'legal_services_regulations',\n",
    "        'corporate_governance',\n",
    "        'market_regulation',\n",
    "        'antitrust_competition_policy'\n",
    "    ],\n",
    "    'Risk Management': [\n",
    "        'insurance_risk_management',\n",
    "        'financial_stability',\n",
    "        'systemic_risk',\n",
    "        'credit_ratings',\n",
    "        'distressed_debt',\n",
    "        'leveraged_buyouts',\n",
    "        'short_selling'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Example usage: Mapping DataFrame columns to themes\n",
    "# Assuming you have a DataFrame `df` with relevant columns\n",
    "# You can create a new column 'Theme' by mapping each existing column to its theme\n",
    "\n",
    "# Sample DataFrame columns\n",
    "df_columns = [\n",
    "    'monetary_policy', 'interest_rates', 'tax_policy',\n",
    "    'economic_growth', 'inflation', 'international_trade',\n",
    "    'energy_resources', 'artificial_intelligence',\n",
    "    'financial_markets_investing', 'environmental_sustainability',\n",
    "    'demographic_shifts_aging', 'geopolitical_events',\n",
    "    'infrastructure', 'labor_market', 'consulting_business_services',\n",
    "    'insurance_risk_management'\n",
    "]\n",
    "\n",
    "# Create a reverse mapping from topic to theme\n",
    "reverse_mapping = {}\n",
    "for theme, topics in theme_mapping.items():\n",
    "    for topic in topics:\n",
    "        reverse_mapping[topic] = theme\n",
    "\n",
    "# Function to map a column to its theme\n",
    "def map_column_to_theme(column):\n",
    "    return reverse_mapping.get(column, 'Uncategorized')\n",
    "\n",
    "# Example DataFrame\n",
    "df = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "# Apply the mapping to assign themes\n",
    "df_mapped = df.columns.to_series().apply(map_column_to_theme).reset_index()\n",
    "df_mapped.columns = ['Column', 'Theme']\n",
    "\n",
    "print(df_mapped)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have your original dataframe 'df_smaller'\n",
    "# Example:\n",
    "# df_smaller = pd.read_csv('your_data.csv', parse_dates=['date'])\n",
    "df_smaller = df_smaller.reset_index()\n",
    "# Create a new dataframe to store the averaged themes\n",
    "df_output = pd.DataFrame()\n",
    "df_output['date'] = df_smaller['date']  # Preserve the date column\n",
    "\n",
    "# Iterate through each theme and compute the mean\n",
    "for theme, columns in theme_mapping.items():\n",
    "    # Check if all columns exist in the dataframe\n",
    "    missing_cols = [col for col in columns if col not in df_smaller.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: The following columns for theme '{theme}' are missing in df_smaller: {missing_cols}\")\n",
    "        # Optionally, you can choose to skip these columns or handle them differently\n",
    "    # Compute the mean, skipping missing columns\n",
    "    available_cols = [col for col in columns if col in df_smaller.columns]\n",
    "    if available_cols:\n",
    "        df_output[theme] = df_smaller[available_cols].mean(axis=1)\n",
    "    else:\n",
    "        df_output[theme] = pd.NA  # Assign NA if no columns are available\n",
    "\n",
    "# Optionally, set 'date' as the index\n",
    "df_output.set_index('date', inplace=True)\n",
    "\n",
    "# Display the resulting dataframe\n",
    "print(df_output.head())\n",
    "\n",
    "\n",
    "df_ranks = df_output.rank(pct=True, axis=1).reset_index()\n",
    "\n",
    "\n",
    "final_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "535675d1-6176-494c-8400-c7ab24afc430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published Chart URL: [{'id': 'standalone', 'url': 'https://www.datawrapper.de/_/9g5G6/', 'name': 'For sharing'}]\n"
     ]
    }
   ],
   "source": [
    "from datawrapper import Datawrapper\n",
    "\n",
    "# First get the order of series from the last row of your dataframe\n",
    "last_row = df_ranks.drop(columns=[\"date\"]).iloc[-1]  # Get the last row\n",
    "# Sort the columns by their values in descending order\n",
    "final_positions = last_row.sort_values(ascending=False).index.tolist()\n",
    "\n",
    "# Create color gradient\n",
    "colors = [\n",
    "    \"#0066CC\",  # Deep blue\n",
    "    \"#1975D1\",\n",
    "    \"#3385D6\",\n",
    "    \"#4D94DB\",\n",
    "    \"#66A3E0\",\n",
    "    \"#80B2E6\",\n",
    "    \"#99C2E9\",\n",
    "    \"#B2D1ED\",\n",
    "    \"#CCE0F3\",\n",
    "    \"#FFE5CC\",\n",
    "    \"#FFCC99\",\n",
    "    \"#FFB366\",\n",
    "    \"#FF9933\",\n",
    "    \"#FF9033\",\n",
    "    \"#FF8000\",  # Deep orange\n",
    "]\n",
    "\n",
    "# Create color mapping based on final positions\n",
    "color_mapping = {series: colors[i] for i, series in enumerate(final_positions)}\n",
    "\n",
    "# Initialize Datawrapper\n",
    "dw = Datawrapper(access_token=\"your_token\")\n",
    "# Create a new line chart\n",
    "chart = dw.create_chart(\n",
    "    title=\"Daily Topic Sentiment Rank\",\n",
    "    chart_type=\"d3-lines\"\n",
    ")\n",
    "\n",
    "# Configure the visualization properties\n",
    "metadata = {\n",
    "    \"visualize\": {\n",
    "        \"dark-mode-invert\": True,\n",
    "        \"interpolation\": \"natural\",\n",
    "        \"x-grid\": \"ticks\",\n",
    "        \"y-grid\": \"off\",\n",
    "        \"opacity\": 1,\n",
    "        \"scale-y\": \"linear\",\n",
    "        \"base-color\": 8,\n",
    "        \"line-width\": 2,\n",
    "        \"label-colors\": True,\n",
    "        \"label-margin\": 138,\n",
    "        \"stack-to-100\": False,\n",
    "        \"show-tooltips\": True,\n",
    "        \"x-grid-format\": \"YYYY-MM-DD\",\n",
    "        \"y-grid-format\": \"auto\",\n",
    "        \"y-grid-labels\": \"auto\",\n",
    "        \"plotHeightMode\": \"ratio\",\n",
    "        \"plotHeightRatio\": 0.62,\n",
    "        \"plotHeightFixed\": 300,\n",
    "        \"color-by-column\": True,\n",
    "        \"connector-lines\": True,\n",
    "        \"y-grid-subdivide\": True,\n",
    "        \"value-label-colors\": True,\n",
    "        \"y-grid-label-align\": \"left\",\n",
    "        \"tooltip\": {\n",
    "            \"sticky\": True,\n",
    "            \"enabled\": True\n",
    "        },\n",
    "        \"lines\": {\n",
    "            series: {\n",
    "                \"color\": color_mapping[series],\n",
    "                \"symbols\": {\"size\": 3, \"style\": \"hollow\", \"enabled\": True}\n",
    "            } for series in final_positions\n",
    "        }\n",
    "    },\n",
    "    \"describe\": {\n",
    "        \"byline\": \"50k websites scraped daily since 2016\",\n",
    "        \"intro\": \"Data derived from daily analysis of 50,000 websites using natural language processing and machine learning algorithms to identify and track topic sentiment.\",\n",
    "        \"source-name\": \"\",\n",
    "        \"source-url\": \"\"\n",
    "    },\n",
    "    \"axes\": {\n",
    "        \"x\": \"date\"\n",
    "    },\n",
    "    \"publish\": {\n",
    "        \"embed-width\": 628,\n",
    "        \"embed-height\": 411,\n",
    "        \"blocks\": {\n",
    "            \"logo\": {\"enabled\": False},\n",
    "            \"embed\": False,\n",
    "            \"download-pdf\": False,\n",
    "            \"download-svg\": False,\n",
    "            \"get-the-data\": True,\n",
    "            \"download-image\": False\n",
    "        },\n",
    "        \"chart-height\": 307\n",
    "    }\n",
    "}\n",
    "\n",
    "# Update the chart with our configuration\n",
    "dw.update_chart(chart['id'], metadata=metadata)\n",
    "# Add the data to the chart\n",
    "dw.add_data(chart['id'], data=df_ranks)\n",
    "# Publish the chart\n",
    "dw.publish_chart(chart['id'])\n",
    "# Get the embed code\n",
    "\n",
    "# Get the published URL\n",
    "published_url = dw.get_chart_display_urls(chart['id'])\n",
    "print(\"Published Chart URL:\", published_url)\n",
    "thematic_url = published_url[0][\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "faf4bf0d-722a-4f89-bd52-21eb59c67be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 'Predict a Mockingbird - 2024-10-29' already exists. Appending new content to it.\n",
      "New content appended successfully.\n",
      "View your page here: https://www.notion.so/12e094f0f3958108a9d2e9f8577120e9\n"
     ]
    }
   ],
   "source": [
    "# Define title\n",
    "page_title = \"Predict a Mockingbird\"\n",
    "\n",
    "# Define content sections using the content_sections dictionary\n",
    "content_sections = {\n",
    "    \"section_1\": {\n",
    "        \"heading\": f\"Ticker, Sector, and Topic Sentiment\",\n",
    "        \"content\": (\n",
    "            \"Tracking ticker-level sentiment allows us to aggregate sentiment to the sectorial level, and we also\"\n",
    "            \" explicity track 98 different themes that we aggregate to main themes for simplicity.\"\n",
    "            \n",
    "        ),\n",
    "        \"url\": news_url,\n",
    "        \"list\": None\n",
    "    },\n",
    "    \"section_2\": {\n",
    "        \"heading\": None,\n",
    "        \"content\": (\n",
    "            \"Here we are tracking the sentiment at a sectorial level which is helpful to sectorial sentiment shifts\"\n",
    "            \n",
    "        ),\n",
    "        \"url\": sector_url,\n",
    "        \"list\": None\n",
    "    },\n",
    "    \"section_3\": {\n",
    "        \"heading\": None,\n",
    "        \"content\": (\n",
    "            \"We also track numerous themes that we aggreagate in a handful of main themes, the detailed themes are available in the SDK.\"\n",
    "            \n",
    "        ),\n",
    "        \"url\": thematic_url,\n",
    "        \"list\": None\n",
    "    },\n",
    "    # Add more sections as needed\n",
    "}\n",
    "\n",
    "# Handle page creation or append\n",
    "handle_page_creation_or_append(page_title, DATABASE_ID, content_sections)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
