{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2761275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sovai[full]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31e1be45-6be8-4bce-915c-8540a3051db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Set up Notion credentials (hardcoded as per your request)\n",
    "NOTION_TOKEN = \"your_notion_token_here\"  # **Ensure this token is kept secure!**\n",
    "DATABASE_ID = \"your_database_id_here\"\n",
    "NOTION_VERSION = \"2022-06-28\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {NOTION_TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Notion-Version\": NOTION_VERSION,\n",
    "}\n",
    "\n",
    "def create_page(title, database_id, children):\n",
    "    \"\"\"\n",
    "    Creates a new page in the specified Notion database.\n",
    "\n",
    "    Args:\n",
    "        title (str): The title of the page.\n",
    "        database_id (str): The ID of the Notion database.\n",
    "        children (list): A list of block objects to include in the page.\n",
    "\n",
    "    Returns:\n",
    "        dict: The response from the Notion API.\n",
    "    \"\"\"\n",
    "    page_data = {\n",
    "        \"parent\": {\"database_id\": database_id},\n",
    "        \"properties\": {\n",
    "            \"Title\": {\n",
    "                \"title\": [\n",
    "                    {\n",
    "                        \"text\": {\n",
    "                            \"content\": title\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "        },\n",
    "        \"children\": children\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.notion.com/v1/pages\", headers=headers, json=page_data)\n",
    "    return response\n",
    "\n",
    "\n",
    "def find_page_by_title(database_id, title):\n",
    "    \"\"\"\n",
    "    Searches the Notion database for a page with the specified title.\n",
    "\n",
    "    Args:\n",
    "        database_id (str): The ID of the Notion database.\n",
    "        title (str): The title to search for.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: The page object if found, else None.\n",
    "    \"\"\"\n",
    "    query_url = f\"https://api.notion.com/v1/databases/{database_id}/query\"\n",
    "    query_data = {\n",
    "        \"filter\": {\n",
    "            \"property\": \"Title\",\n",
    "            \"title\": {\n",
    "                \"equals\": title\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(query_url, headers=headers, json=query_data)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to query database:\")\n",
    "        print(json.dumps(response.json(), indent=2))\n",
    "        return None\n",
    "\n",
    "    results = response.json().get(\"results\")\n",
    "    if results:\n",
    "        return results[0]  # Assuming titles are unique\n",
    "    return None\n",
    "\n",
    "\n",
    "def append_to_page(page_id, children):\n",
    "    \"\"\"\n",
    "    Appends new blocks to an existing Notion page.\n",
    "\n",
    "    Args:\n",
    "        page_id (str): The ID of the page to append to.\n",
    "        children (list): A list of block objects to append.\n",
    "\n",
    "    Returns:\n",
    "        dict: The response from the Notion API.\n",
    "    \"\"\"\n",
    "    append_url = f\"https://api.notion.com/v1/blocks/{page_id}/children\"\n",
    "    append_data = {\n",
    "        \"children\": children\n",
    "    }\n",
    "    response = requests.patch(append_url, headers=headers, json=append_data)\n",
    "    return response\n",
    "\n",
    "\n",
    "def build_content_from_dict(content_dict):\n",
    "    \"\"\"\n",
    "    Builds Notion content blocks from a dictionary.\n",
    "\n",
    "    Args:\n",
    "        content_dict (dict): A dictionary containing content definitions.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Notion block objects.\n",
    "    \"\"\"\n",
    "    children = []\n",
    "\n",
    "    # Add Heading\n",
    "    if \"heading\" in content_dict and content_dict[\"heading\"]:\n",
    "        children.append(\n",
    "            {\n",
    "                \"object\": \"block\",\n",
    "                \"type\": \"heading_2\",\n",
    "                \"heading_2\": {\n",
    "                    \"rich_text\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": {\n",
    "                                \"content\": content_dict[\"heading\"]\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Add Content\n",
    "    if \"content\" in content_dict and content_dict[\"content\"]:\n",
    "        children.append(\n",
    "            {\n",
    "                \"object\": \"block\",\n",
    "                \"type\": \"paragraph\",\n",
    "                \"paragraph\": {\n",
    "                    \"rich_text\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": {\n",
    "                                \"content\": content_dict[\"content\"]\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Add List Items (Bullet Points)\n",
    "    if \"list\" in content_dict and content_dict[\"list\"]:\n",
    "        list_blocks = build_bullet_list(content_dict[\"list\"])\n",
    "        children.extend(list_blocks)\n",
    "        \n",
    "    # Add URL as a Link\n",
    "    if \"url\" in content_dict and content_dict[\"url\"]:\n",
    "        children.append(\n",
    "            {\n",
    "                \"object\": \"block\",\n",
    "                \"type\": \"paragraph\",\n",
    "                \"paragraph\": {\n",
    "                    \"rich_text\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": {\n",
    "                                \"content\": content_dict[\"url\"],\n",
    "                                \"link\": {\"url\": content_dict[\"url\"]}\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    return children\n",
    "\n",
    "\n",
    "def build_bullet_list(items):\n",
    "    \"\"\"\n",
    "    Builds Notion bullet list blocks from a list of items.\n",
    "\n",
    "    Args:\n",
    "        items (list): A list of strings representing bullet points.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Notion bulleted list item block objects.\n",
    "    \"\"\"\n",
    "    bullet_blocks = []\n",
    "    for item in items:\n",
    "        bullet_blocks.append(\n",
    "            {\n",
    "                \"object\": \"block\",\n",
    "                \"type\": \"bulleted_list_item\",\n",
    "                \"bulleted_list_item\": {\n",
    "                    \"rich_text\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": {\n",
    "                                \"content\": item\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "    return bullet_blocks\n",
    "\n",
    "\n",
    "def build_children_from_sections(content_sections):\n",
    "    \"\"\"\n",
    "    Iterates through the content sections dictionary and builds the children blocks.\n",
    "\n",
    "    Args:\n",
    "        content_sections (dict): Dictionary containing all content sections.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Notion block objects.\n",
    "    \"\"\"\n",
    "    children = []\n",
    "    for key in sorted(content_sections.keys()):\n",
    "        section = content_sections[key]\n",
    "        section_blocks = build_content_from_dict(section)\n",
    "        children.extend(section_blocks)\n",
    "    return children\n",
    "\n",
    "\n",
    "def handle_page_creation_or_append(title, database_id, content_sections):\n",
    "    \"\"\"\n",
    "    Handles the logic to either create a new page or append content to an existing page.\n",
    "\n",
    "    Args:\n",
    "        title (str): The title of the page.\n",
    "        database_id (str): The ID of the Notion database.\n",
    "        content_sections (dict): Dictionary containing all content sections.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    full_title = f\"{title} - {current_date}\"\n",
    "\n",
    "    # Build the content blocks\n",
    "    children = build_children_from_sections(content_sections)\n",
    "\n",
    "    # Check if the page already exists\n",
    "    existing_page = find_page_by_title(database_id, full_title)\n",
    "\n",
    "    if existing_page:\n",
    "        print(f\"Page '{full_title}' already exists. Appending new content to it.\")\n",
    "        page_id = existing_page[\"id\"]\n",
    "        response = append_to_page(page_id, children)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"New content appended successfully.\")\n",
    "            # Construct the page URL manually\n",
    "            # Note: Notion page URLs follow the format https://www.notion.so/{workspace}/{page_id}\n",
    "            # However, constructing the exact URL might require additional steps.\n",
    "            # Here, we'll provide a placeholder.\n",
    "            page_url = f\"https://www.notion.so/{page_id.replace('-', '')}\"\n",
    "            print(f\"View your page here: {page_url}\")\n",
    "        else:\n",
    "            print(\"Failed to append new content:\")\n",
    "            print(json.dumps(response.json(), indent=2))\n",
    "    else:\n",
    "        print(f\"Page '{full_title}' does not exist. Creating a new page with the new content.\")\n",
    "        response = create_page(full_title, database_id, children)\n",
    "        \n",
    "        # Handle the response\n",
    "        if response.status_code == 200:\n",
    "            page_url = response.json().get(\"url\", \"No URL returned\")\n",
    "            print(\"Page created successfully with the new content.\")\n",
    "            print(f\"View your page here: {page_url}\")\n",
    "        else:\n",
    "            print(\"Failed to create page:\")\n",
    "            print(json.dumps(response.json(), indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a4b18b5-4c3f-4662-b1c5-3b9e07098441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sovai as sov\n",
    "import pandas as pd\n",
    "\n",
    "sov.token_auth(token=\"visit https://sov.ai/profile for your token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5df9537-d119-4b7e-a1ec-871d4012e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_meta = pd.read_parquet(\"data/tickers.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "428ba5c1-846f-456c-ac57-0d7af125fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki = sov.data(\"wikipedia/views\", full_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b14c25c1-5bf6-4e66-804c-e433913e5c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ticker       date  search_pressure  search_past  search_change  \\\n",
      "0      A 2024-08-05            0.558          NaN            NaN   \n",
      "1      A 2024-08-12            0.535        0.558         -0.022   \n",
      "2      A 2024-08-19            0.477        0.535         -0.059   \n",
      "3      A 2024-08-26            0.537        0.477          0.060   \n",
      "4      A 2024-09-02            0.567        0.537          0.031   \n",
      "\n",
      "   search_quarter  lt_change  \n",
      "0           0.562     -0.005  \n",
      "1           0.562     -0.027  \n",
      "2           0.562     -0.086  \n",
      "3           0.562     -0.026  \n",
      "4           0.562      0.005  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================\n",
    "# Step 1: Data Preparation\n",
    "# ================================\n",
    "\n",
    "# Reset index to convert 'ticker' and 'date' from MultiIndex to columns\n",
    "df_wiki = df_wiki.reset_index()\n",
    "\n",
    "# Ensure 'date' is in datetime format\n",
    "df_wiki['date'] = pd.to_datetime(df_wiki['date'])\n",
    "\n",
    "\n",
    "# Keep only rows where 'date' is within [start_date, max_date]\n",
    "df_wiki = df_wiki[df_wiki['date'] >= (df_wiki[\"date\"].max() - pd.Timedelta(weeks=12))]\n",
    "\n",
    "\n",
    "\n",
    "# Sort the DataFrame by 'ticker' and 'date' to maintain chronological order\n",
    "df_daily = df_wiki.sort_values(['ticker', 'date']).copy()\n",
    "\n",
    "# Calculate 'previous_day_search_pressure' by shifting 'search_pressure' by one day within each 'ticker'\n",
    "df_daily['prev_day'] = df_daily.groupby('ticker')['search_pressure'].shift(1)\n",
    "\n",
    "# Calculate 'daily_change' as the difference between current and previous search pressures\n",
    "df_daily['day_chg'] = df_daily['search_pressure'] - df_daily['prev_day']\n",
    "\n",
    "# Calculate 'average_search_quarter' as the average 'search_pressure' over the last 12 weeks per 'ticker'\n",
    "df_daily['avg_day'] = df_daily.groupby('ticker')['search_pressure'].transform('mean')\n",
    "\n",
    "# Calculate 'long_term_change' as the difference between current 'search_pressure' and 'average_search_quarter'\n",
    "df_daily['ma_day'] = df_daily['search_pressure'] - df_daily['avg_day']\n",
    "\n",
    "# Drop rows with NaN values (e.g., first day per ticker where 'previous_search_pressure' is NaN)\n",
    "# Filter to include only rows where 'date' == 'max_date_per_ticker' per ticker\n",
    "df_daily = df_daily[df_daily['date'] == df_wiki[\"date\"].max() ]\n",
    "\n",
    "df_daily = df_daily.rename(columns={\"search_pressure\":\"day_search\"})\n",
    "\n",
    "# Select relevant columns\n",
    "df_daily = df_daily[['ticker', 'date', 'day_search',  \n",
    "                     'day_chg', 'ma_day']]\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_wiki = df_wiki.set_index([\"ticker\",\"date\"])\n",
    "\n",
    "# Step 1: Find the global maximum date\n",
    "max_date = df_wiki.index.get_level_values('date').max()\n",
    "\n",
    "# Step 2: Get the weekday abbreviation (e.g., 'TUE' for Tuesday)\n",
    "weekday_abbr = max_date.strftime('%a').upper()[:3]\n",
    "\n",
    "# Define the resampling frequency to end on the max_date's weekday\n",
    "resample_freq = f'W-{weekday_abbr}'\n",
    "\n",
    "df_wiki = (\n",
    "    df_wiki\n",
    "    .groupby('ticker')\n",
    "    .resample(resample_freq, level='date')['search_pressure']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "# Step 1: Sort the DataFrame by 'ticker' and 'date'\n",
    "df_wiki = df_wiki.sort_values(['ticker', 'date'])\n",
    "\n",
    "\n",
    "# Step 3: Create 'search_past' by shifting 'search_pressure' by one week within each 'ticker'\n",
    "df_wiki['search_past'] = df_wiki.groupby('ticker')['search_pressure'].shift(1)\n",
    "\n",
    "# Step 4: Calculate 'search_change' as the difference between current and past search pressures\n",
    "df_wiki['search_change'] = df_wiki['search_pressure'] - df_wiki['search_past']\n",
    "\n",
    "# Optional: Handle NaN values if desired\n",
    "# df_wiki['search_past'] = df_wiki['search_past'].fillna(0)\n",
    "# df_wiki['search_change'] = df_wiki['search_change'].fillna(0)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_wiki is already processed up to the previous steps\n",
    "\n",
    "# Step 1: Calculate 'search_quarter' as the average 'search_pressure' over the 12 weeks per 'ticker'\n",
    "df_wiki['search_quarter'] = df_wiki.groupby('ticker')['search_pressure'].transform('mean')\n",
    "\n",
    "# Step 2: Calculate 'lt_change' as the difference between 'search_pressure' and 'search_quarter'\n",
    "df_wiki['lt_change'] = df_wiki['search_pressure'] - df_wiki['search_quarter']\n",
    "\n",
    "# Optional: If you prefer to round the results for better readability\n",
    "# df_wiki['search_quarter'] = df_wiki['search_quarter'].round(3)\n",
    "# df_wiki['lt_change'] = df_wiki['lt_change'].round(3)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df_wiki.head())\n",
    "\n",
    "\n",
    "df_wiki = df_wiki.dropna()\n",
    "\n",
    "\n",
    "\n",
    "# Filter out rows where 'date' is the max date for each 'ticker'\n",
    "df_wiki = df_wiki[df_wiki['date'] == df_wiki[\"date\"].max()]\n",
    "\n",
    "\n",
    "df_wiki.sort_values(\"search_change\")\n",
    "\n",
    "df_wiki = df_wiki.drop(columns=[\"search_quarter\"])\n",
    "\n",
    "df_wiki = df_wiki.rename(columns={\"search_pressure\":\"search\", \"search_past\":\"last\",\"search_change\":\"change\",\"lt_change\":\"ltavg\"})\n",
    "\n",
    "df_wiki = pd.merge(df_wiki.set_index([\"ticker\",\"date\"]), df_daily.set_index([\"ticker\",\"date\"]),left_index=True, right_index=True)\n",
    "\n",
    "df_wiki.head()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Rename columns\n",
    "df_wiki = df_wiki.rename(columns={\n",
    "    'day_chg': 'day change',\n",
    "    'change': 'week change',\n",
    "    'ma_day': 'short',\n",
    "    'ltavg': 'long'\n",
    "})\n",
    "\n",
    "# Reorder columns with day metrics first, then week metrics\n",
    "new_order = ['search', 'last', 'week change', 'long', 'day change', 'short']\n",
    "\n",
    "df_wiki = df_wiki[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bb86ac8-f1fc-4cd9-a619-ebc7faaf44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_org = sov.data(\"wikipedia/views\", full_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fb91fd4-012b-4b9a-abc0-23ad57a046d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pressure columns kept and their order: ['pressure_0', 'pressure_1', 'pressure_5', 'pressure_9', 'pressure_13', 'pressure_17', 'pressure_21', 'pressure_25', 'pressure_29', 'pressure_33', 'pressure_37', 'pressure_41', 'pressure_45', 'pressure_49', 'pressure_53', 'pressure_57', 'pressure_60']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_pressure_history_columns(df_org, df_wiki, lookback=60):\n",
    "    \"\"\"\n",
    "    Creates pressure history columns from df_org and merges them with df_wiki.\n",
    "\n",
    "    Parameters:\n",
    "    - df_org (pd.DataFrame): Original DataFrame with a MultiIndex including 'date'.\n",
    "    - df_wiki (pd.DataFrame): DataFrame to merge the pressure columns into.\n",
    "    - lookback (int): Number of days to look back for pressure data.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Merged DataFrame with pressure history columns.\n",
    "    \"\"\"\n",
    "    # 1. Get the maximum date from index level 'date'\n",
    "    max_date = df_org.index.get_level_values('date').max()\n",
    "    \n",
    "    # 2. Calculate the cutoff date\n",
    "    cutoff_date = max_date - pd.Timedelta(days=lookback)\n",
    "    \n",
    "    # 3. Filter df_org to only include data after cutoff date\n",
    "    df_filtered = df_org[df_org.index.get_level_values('date') >= cutoff_date]\n",
    "    \n",
    "    # 4. Create pivot table with filtered data\n",
    "    df_pivot = df_filtered.reset_index().pivot(\n",
    "        index='ticker', \n",
    "        columns='date', \n",
    "        values='search_pressure'\n",
    "    )\n",
    "    \n",
    "    # 5. Sort the columns by date ascendingly to ensure pressure_0 is the earliest\n",
    "    df_pivot = df_pivot.sort_index(axis=1)\n",
    "    \n",
    "    # 6. Create pressure column names in ascending order\n",
    "    num_cols = len(df_pivot.columns)\n",
    "    all_pressure_cols = [f'pressure_{i}' for i in range(num_cols)]\n",
    "    df_pivot.columns = all_pressure_cols\n",
    "    \n",
    "    # 7. Select columns: first, every 4th, and last\n",
    "    pressure_cols_to_keep = [all_pressure_cols[0]]  # First column (pressure_0)\n",
    "    if len(all_pressure_cols) > 2:  # If we have middle columns\n",
    "        pressure_cols_to_keep.extend(all_pressure_cols[1:-1:4])  # Every 4th column\n",
    "    pressure_cols_to_keep.append(all_pressure_cols[-1])  # Last column (pressure_{num_cols-1})\n",
    "    \n",
    "    # 8. Keep only selected columns\n",
    "    df_pivot = df_pivot[pressure_cols_to_keep]\n",
    "    \n",
    "    # 9. Reset index to make ticker a column\n",
    "    df_pivot = df_pivot.reset_index()\n",
    "    \n",
    "    # 10. Merge with original df_wiki\n",
    "    df_wiki_expanded = df_wiki.merge(df_pivot, on='ticker', how='left')\n",
    "    \n",
    "    return df_wiki_expanded\n",
    "\n",
    "# Example Usage:\n",
    "\n",
    "# Assuming you have df_org and df_wiki already defined\n",
    "# df_org should have a MultiIndex with 'date' and 'ticker'\n",
    "# df_wiki is the DataFrame you want to expand with pressure columns\n",
    "\n",
    "# Apply the function\n",
    "df_wiki_expanded = create_pressure_history_columns(df_org, df_wiki)\n",
    "\n",
    "# To check the columns we kept and their order\n",
    "pressure_columns = [col for col in df_wiki_expanded.columns if col.startswith('pressure_')]\n",
    "print(\"Pressure columns kept and their order:\", pressure_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7abbcd30-46b2-44b6-a00e-8522d90cb71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki_expanded[[\"search\",\"last\",\"week change\",\"long\",\"day change\",\"short\"]] = df_wiki_expanded[[\"search\",\"last\",\"week change\",\"long\",\"day change\",\"short\"]]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7108c55-4a13-4724-ac87-eb1d49b9a5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>search</th>\n",
       "      <th>last</th>\n",
       "      <th>week change</th>\n",
       "      <th>day change</th>\n",
       "      <th>long</th>\n",
       "      <th>pressure_0</th>\n",
       "      <th>pressure_1</th>\n",
       "      <th>pressure_5</th>\n",
       "      <th>pressure_9</th>\n",
       "      <th>pressure_13</th>\n",
       "      <th>pressure_17</th>\n",
       "      <th>pressure_21</th>\n",
       "      <th>pressure_25</th>\n",
       "      <th>pressure_29</th>\n",
       "      <th>pressure_33</th>\n",
       "      <th>pressure_37</th>\n",
       "      <th>pressure_41</th>\n",
       "      <th>pressure_45</th>\n",
       "      <th>pressure_49</th>\n",
       "      <th>pressure_53</th>\n",
       "      <th>pressure_57</th>\n",
       "      <th>pressure_60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MD</td>\n",
       "      <td>4.983</td>\n",
       "      <td>2.539</td>\n",
       "      <td>2.444</td>\n",
       "      <td>0.990</td>\n",
       "      <td>-22.102</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>GIC</td>\n",
       "      <td>30.874</td>\n",
       "      <td>57.671</td>\n",
       "      <td>-26.797</td>\n",
       "      <td>-2.680</td>\n",
       "      <td>-33.709</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>LORL</td>\n",
       "      <td>22.266</td>\n",
       "      <td>47.264</td>\n",
       "      <td>-24.999</td>\n",
       "      <td>-1.701</td>\n",
       "      <td>-34.630</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>JMBA</td>\n",
       "      <td>32.371</td>\n",
       "      <td>20.394</td>\n",
       "      <td>11.977</td>\n",
       "      <td>5.393</td>\n",
       "      <td>7.974</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GEF</td>\n",
       "      <td>84.398</td>\n",
       "      <td>85.140</td>\n",
       "      <td>-0.742</td>\n",
       "      <td>-1.060</td>\n",
       "      <td>19.381</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>LPT</td>\n",
       "      <td>70.053</td>\n",
       "      <td>52.973</td>\n",
       "      <td>17.080</td>\n",
       "      <td>0.793</td>\n",
       "      <td>44.256</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>COLD</td>\n",
       "      <td>31.257</td>\n",
       "      <td>53.122</td>\n",
       "      <td>-21.865</td>\n",
       "      <td>-1.697</td>\n",
       "      <td>-29.492</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>HUT</td>\n",
       "      <td>72.563</td>\n",
       "      <td>62.754</td>\n",
       "      <td>9.809</td>\n",
       "      <td>-0.461</td>\n",
       "      <td>41.726</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>MLNK</td>\n",
       "      <td>45.932</td>\n",
       "      <td>54.802</td>\n",
       "      <td>-8.870</td>\n",
       "      <td>-3.795</td>\n",
       "      <td>-3.091</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>MCD</td>\n",
       "      <td>65.934</td>\n",
       "      <td>40.083</td>\n",
       "      <td>25.851</td>\n",
       "      <td>2.212</td>\n",
       "      <td>23.887</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ticker  search   last  week change  day change    long  pressure_0  \\\n",
       "18      MD   4.983  2.539        2.444       0.990 -22.102       0.555   \n",
       "45     GIC  30.874 57.671      -26.797      -2.680 -33.709       0.470   \n",
       "47    LORL  22.266 47.264      -24.999      -1.701 -34.630       0.447   \n",
       "89    JMBA  32.371 20.394       11.977       5.393   7.974       0.446   \n",
       "4      GEF  84.398 85.140       -0.742      -1.060  19.381       0.582   \n",
       "..     ...     ...    ...          ...         ...     ...         ...   \n",
       "115    LPT  70.053 52.973       17.080       0.793  44.256       0.262   \n",
       "59    COLD  31.257 53.122      -21.865      -1.697 -29.492       0.518   \n",
       "63     HUT  72.563 62.754        9.809      -0.461  41.726       0.223   \n",
       "114   MLNK  45.932 54.802       -8.870      -3.795  -3.091       0.139   \n",
       "37     MCD  65.934 40.083       25.851       2.212  23.887       0.685   \n",
       "\n",
       "     pressure_1  pressure_5  pressure_9  pressure_13  pressure_17  \\\n",
       "18        0.560       0.473       0.528        0.635        0.663   \n",
       "45        0.488       0.529       0.605        0.667        0.739   \n",
       "47        0.450       0.522       0.592        0.580        0.637   \n",
       "89        0.444       0.462       0.533        0.633        0.626   \n",
       "4         0.618       0.706       0.702        0.637        0.612   \n",
       "..          ...         ...         ...          ...          ...   \n",
       "115       0.276       0.309       0.245        0.176        0.160   \n",
       "59        0.510       0.440       0.443        0.459        0.451   \n",
       "63        0.204       0.175       0.153        0.173        0.182   \n",
       "114       0.143       0.157       0.150        0.189        0.244   \n",
       "37        0.661       0.615       0.565        0.525        0.424   \n",
       "\n",
       "     pressure_21  pressure_25  pressure_29  pressure_33  pressure_37  \\\n",
       "18         0.505        0.294        0.167        0.095        0.055   \n",
       "45         0.794        0.798        0.732        0.651        0.614   \n",
       "47         0.748        0.810        0.772        0.672        0.598   \n",
       "89         0.526        0.489        0.470        0.451        0.400   \n",
       "4          0.638        0.644        0.674        0.712        0.742   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "115        0.163        0.209        0.297        0.339        0.355   \n",
       "59         0.444        0.376        0.392        0.429        0.561   \n",
       "63         0.181        0.241        0.274        0.318        0.318   \n",
       "114        0.314        0.470        0.588        0.689        0.701   \n",
       "37         0.278        0.265        0.266        0.273        0.176   \n",
       "\n",
       "     pressure_41  pressure_45  pressure_49  pressure_53  pressure_57  \\\n",
       "18         0.032        0.023        0.023        0.032        0.047   \n",
       "45         0.628        0.668        0.622        0.457        0.304   \n",
       "47         0.559        0.535        0.515        0.363        0.212   \n",
       "89         0.348        0.279        0.205        0.182        0.315   \n",
       "4          0.786        0.828        0.852        0.852        0.852   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "115        0.351        0.396        0.504        0.612        0.709   \n",
       "59         0.656        0.659        0.563        0.432        0.310   \n",
       "63         0.381        0.474        0.600        0.707        0.726   \n",
       "114        0.619        0.562        0.538        0.557        0.461   \n",
       "37         0.187        0.274        0.364        0.509        0.667   \n",
       "\n",
       "     pressure_60  \n",
       "18         0.072  \n",
       "45         0.214  \n",
       "47         0.153  \n",
       "89         0.490  \n",
       "4          0.823  \n",
       "..           ...  \n",
       "115        0.745  \n",
       "59         0.241  \n",
       "63         0.719  \n",
       "114        0.355  \n",
       "37         0.744  \n",
       "\n",
       "[100 rows x 23 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "columns_to_process = ['search', 'week change', 'long', 'day change', 'short']\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "top_bottom_dfs = []\n",
    "\n",
    "# Iterate over each column\n",
    "for col in columns_to_process:\n",
    "    # Ensure the column exists in the DataFrame\n",
    "    if col not in df_wiki_expanded.columns:\n",
    "        print(f\"Column '{col}' does not exist in the DataFrame.\")\n",
    "        continue\n",
    "\n",
    "    # Sort ascending to get bottom 15\n",
    "    bottom_15 = df_wiki_expanded.sort_values(by=col, ascending=True).head(15).copy()\n",
    "    \n",
    "    # Sort descending to get top 15\n",
    "    top_15 = df_wiki_expanded.sort_values(by=col, ascending=False).head(15).copy()\n",
    "    \n",
    "    # Append to the list\n",
    "    top_bottom_dfs.extend([top_15, bottom_15])\n",
    "\n",
    "# Concatenate all DataFrames in the list\n",
    "combined_df = pd.concat(top_bottom_dfs, ignore_index=True)\n",
    "\n",
    "# Remove duplicate rows\n",
    "combined_df_unique = combined_df.drop_duplicates()\n",
    "\n",
    "# Reset index for cleanliness\n",
    "combined_df_unique.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "combined_df_unique = combined_df_unique.sample(100)\n",
    "\n",
    "combined_df_unique = combined_df_unique.drop(columns=[\"long\"])\n",
    "\n",
    "combined_df_unique = combined_df_unique.rename(columns={\"short\":\"long\"})\n",
    "\n",
    "combined_df_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "430e04e7-434b-407d-b047-c5572980fb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published Chart URL: [{'id': 'standalone', 'url': 'https://www.datawrapper.de/_/aM1xQ/', 'name': 'For sharing'}]\n"
     ]
    }
   ],
   "source": [
    "from datawrapper import Datawrapper\n",
    "\n",
    "# Initialize Datawrapper\n",
    "dw = Datawrapper(access_token=\"your_token\")\n",
    "\n",
    "# Create the chart\n",
    "chart = dw.create_chart(\n",
    "    title=\"Stock Wikipedia Views Analysis\",\n",
    "    chart_type=\"tables\"\n",
    ")\n",
    "\n",
    "# Add the data to the chart\n",
    "dw.add_data(chart['id'], data=combined_df_unique)\n",
    "\n",
    "# Get pressure column names\n",
    "pressure_cols = sorted([col for col in combined_df_unique.columns if col.startswith('pressure_')])\n",
    "\n",
    "# Configure the visualization properties\n",
    "properties = {\n",
    "     \"visualize\": {\n",
    "        \"dark-mode-invert\": True,\n",
    "        \"perPage\": 20,\n",
    "        \"columns\": {\n",
    "            \"ticker\": {\n",
    "                \"align\": \"left\",\n",
    "                \"title\": \"Stock\",\n",
    "                \"width\": \"100\"\n",
    "            },\n",
    "            \"search\": {\n",
    "                \"title\": \"Search\",\n",
    "                \"format\": \"0.000\",\n",
    "                \"width\": \"120\"\n",
    "            },\n",
    "            \"last\": {\n",
    "                \"title\": \"Last\",\n",
    "                \"format\": \"0.000\",\n",
    "                \"width\": \"120\"\n",
    "            },\n",
    "            \"week change\": {\n",
    "                \"title\": \"Change\",\n",
    "                \"format\": \"+0.000\",\n",
    "                \"width\": 0.27,  # Updated to match working example\n",
    "                \"showAsBar\": True,\n",
    "                \"barColorNegative\": \"#ff4444\",\n",
    "                \"fixedWidth\": True\n",
    "            },\n",
    "            \"long\": {\n",
    "                \"title\": \"Long Trend\",\n",
    "                \"format\": \"+0.000\",\n",
    "                \"width\": \"120\"\n",
    "            },\n",
    "            \"day change\": {\n",
    "                \"title\": \"Day Change\",\n",
    "                \"format\": \"+0.000\",\n",
    "                \"width\": 0.17,  # Updated to match working example\n",
    "                \"fixedWidth\": True\n",
    "            },\n",
    "            \"pressure_0\": {\n",
    "                \"type\": \"number\",\n",
    "                \"title\": \"History\",  # Updated to just \"History\"\n",
    "                \"width\": 0.33,  # Updated to match working example\n",
    "                \"format\": \"0.000\",\n",
    "                \"sparkline\": {\n",
    "                    \"color\": \"#18a1cd\",\n",
    "                    \"title\": \"History\",\n",
    "                    \"enabled\": True,\n",
    "                    \"stroke\": 2,\n",
    "                    \"dotMax\": True,\n",
    "                    \"dotMin\": True,\n",
    "                    \"dotFirst\": True,\n",
    "                    \"dotLast\": True\n",
    "                },\n",
    "                \"fixedWidth\": True\n",
    "            }\n",
    "        },\n",
    "        \"header\": {\n",
    "            \"style\": {\n",
    "                \"bold\": True,\n",
    "                \"fontSize\": 0.9,\n",
    "                \"color\": \"#494949\"\n",
    "            },\n",
    "            \"borderBottom\": \"2px\",\n",
    "            \"borderBottomColor\": \"#333333\"\n",
    "        },\n",
    "        \"pagination\": {\n",
    "            \"enabled\": True,\n",
    "            \"position\": \"bottom\",\n",
    "            \"pagesPerScreen\": 10\n",
    "        },\n",
    "        \"striped\": True,\n",
    "        \"markdown\": True,\n",
    "        \"showHeader\": True,\n",
    "        \"compactMode\": True,\n",
    "        \"firstRowIsHeader\": False,\n",
    "        \"firstColumnIsSticky\": False,\n",
    "        \"mergeEmptyCells\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "# Configure remaining pressure columns exactly like the first one\n",
    "for col in pressure_cols[1:]:\n",
    "    properties[\"visualize\"][\"columns\"][col] = {\n",
    "        \"type\": \"number\",\n",
    "        \"width\": 0.33,  # Updated to match working example\n",
    "        \"format\": \"0.000\",\n",
    "        \"sparkline\": {\n",
    "            \"color\": \"#18a1cd\",\n",
    "            \"title\": \"pressure_history\",\n",
    "            \"enabled\": True\n",
    "        },\n",
    "        \"fixedWidth\": True\n",
    "    }\n",
    "\n",
    "# Set column order\n",
    "properties[\"visualize\"][\"column-order\"] = [\n",
    "    \"ticker\",\n",
    "    \"search\",\n",
    "    \"last\",\n",
    "    \"weel change\",\n",
    "    \"long\",\n",
    "    \"day change\",\n",
    "] + pressure_cols\n",
    "\n",
    "# Add other visualization settings\n",
    "from datetime import datetime\n",
    "\n",
    "current_date = datetime.now().strftime(\"%B %d, %Y\")\n",
    "\n",
    "# Add other visualization settings\n",
    "properties[\"describe\"] = {\n",
    "    \"intro\": f\"\"\"Analysis of Wikipedia page views for stocks with historical pressure trends over 60 days, as of {current_date}. \n",
    "    This analysis tracks market sentiment through public interest patterns. \n",
    "    Derived from <a href='https://docs.sov.ai/realtime-datasets/equity-datasets/wikipedia-views'>Sov.ai™ Wiki</a> datasets. \"\"\",\n",
    "    \"byline\": \"\",\n",
    "    \"source-name\": \"Wikipedia Views Data\",\n",
    "    \"source-url\": \"\",\n",
    "    \"hide-title\": False\n",
    "}\n",
    "\n",
    "properties[\"publish\"] = {\n",
    "    \"embed-width\": 682,\n",
    "    \"embed-height\": 1086,\n",
    "    \"blocks\": {\n",
    "        \"logo\": {\"enabled\": False},\n",
    "        \"embed\": False,\n",
    "        \"download-pdf\": False,\n",
    "        \"download-svg\": False,\n",
    "        \"get-the-data\": True,\n",
    "        \"download-image\": False\n",
    "    },\n",
    "    \"autoDarkMode\": False,\n",
    "    \"chart-height\": 988,\n",
    "    \"force-attribution\": False\n",
    "}\n",
    "\n",
    "# Update the chart with the properties\n",
    "dw.update_chart(\n",
    "    chart['id'],\n",
    "    metadata=properties\n",
    ")\n",
    "\n",
    "# Publish the chart\n",
    "dw.publish_chart(chart['id'])\n",
    "\n",
    "# Get the published URL\n",
    "published_url = dw.get_chart_display_urls(chart['id'])\n",
    "print(\"Published Chart URL:\", published_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d4d627f-c1f6-4c90-8ce7-82a5e88a6f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_url = published_url[0][\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58f30317-6284-4627-9c97-1128d484bef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 'Predict a Mockingbird - 2024-10-29' already exists. Appending new content to it.\n",
      "New content appended successfully.\n",
      "View your page here: https://www.notion.so/12e094f0f3958108a9d2e9f8577120e9\n"
     ]
    }
   ],
   "source": [
    "# Define title\n",
    "page_title = \"Predict a Mockingbird\"\n",
    "\n",
    "# Define content sections using the content_sections dictionary\n",
    "content_sections = {\n",
    "    \"section_1\": {\n",
    "        \"heading\": f\"Wikipedia Search Pressure\",\n",
    "        \"content\": (\n",
    "            \"This model uses an algorithm to discover early search pressure on wikipedia pages.\"\n",
    "            \" This signal is stronger than most retail signals because it reflexts the start of a true research process.\"\n",
    "            \n",
    "        ),\n",
    "        \"url\": wiki_url,\n",
    "        \"list\": None\n",
    "    },\n",
    "\n",
    "    # Add more sections as needed\n",
    "}\n",
    "\n",
    "# Handle page creation or append\n",
    "handle_page_creation_or_append(page_title, DATABASE_ID, content_sections)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
