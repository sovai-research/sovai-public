{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d2027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sovai[full]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6abe0b34-9eff-4cc9-a503-af552e2f09b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Set up Notion credentials (hardcoded as per your request)\n",
    "NOTION_TOKEN = \"your_notion_token_here\"  # **Ensure this token is kept secure!**\n",
    "DATABASE_ID = \"your_database_id_here\"\n",
    "NOTION_VERSION = \"2022-06-28\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {NOTION_TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Notion-Version\": NOTION_VERSION,\n",
    "}\n",
    "\n",
    "def create_page(title, database_id, children):\n",
    "    \"\"\"\n",
    "    Creates a new page in the specified Notion database.\n",
    "\n",
    "    Args:\n",
    "        title (str): The title of the page.\n",
    "        database_id (str): The ID of the Notion database.\n",
    "        children (list): A list of block objects to include in the page.\n",
    "\n",
    "    Returns:\n",
    "        dict: The response from the Notion API.\n",
    "    \"\"\"\n",
    "    page_data = {\n",
    "        \"parent\": {\"database_id\": database_id},\n",
    "        \"properties\": {\n",
    "            \"Title\": {\n",
    "                \"title\": [\n",
    "                    {\n",
    "                        \"text\": {\n",
    "                            \"content\": title\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "        },\n",
    "        \"children\": children\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.notion.com/v1/pages\", headers=headers, json=page_data)\n",
    "    return response\n",
    "\n",
    "\n",
    "def find_page_by_title(database_id, title):\n",
    "    \"\"\"\n",
    "    Searches the Notion database for a page with the specified title.\n",
    "\n",
    "    Args:\n",
    "        database_id (str): The ID of the Notion database.\n",
    "        title (str): The title to search for.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: The page object if found, else None.\n",
    "    \"\"\"\n",
    "    query_url = f\"https://api.notion.com/v1/databases/{database_id}/query\"\n",
    "    query_data = {\n",
    "        \"filter\": {\n",
    "            \"property\": \"Title\",\n",
    "            \"title\": {\n",
    "                \"equals\": title\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(query_url, headers=headers, json=query_data)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to query database:\")\n",
    "        print(json.dumps(response.json(), indent=2))\n",
    "        return None\n",
    "\n",
    "    results = response.json().get(\"results\")\n",
    "    if results:\n",
    "        return results[0]  # Assuming titles are unique\n",
    "    return None\n",
    "\n",
    "\n",
    "def append_to_page(page_id, children):\n",
    "    \"\"\"\n",
    "    Appends new blocks to an existing Notion page.\n",
    "\n",
    "    Args:\n",
    "        page_id (str): The ID of the page to append to.\n",
    "        children (list): A list of block objects to append.\n",
    "\n",
    "    Returns:\n",
    "        dict: The response from the Notion API.\n",
    "    \"\"\"\n",
    "    append_url = f\"https://api.notion.com/v1/blocks/{page_id}/children\"\n",
    "    append_data = {\n",
    "        \"children\": children\n",
    "    }\n",
    "    response = requests.patch(append_url, headers=headers, json=append_data)\n",
    "    return response\n",
    "\n",
    "\n",
    "def build_content_from_dict(content_dict):\n",
    "    \"\"\"\n",
    "    Builds Notion content blocks from a dictionary.\n",
    "\n",
    "    Args:\n",
    "        content_dict (dict): A dictionary containing content definitions.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Notion block objects.\n",
    "    \"\"\"\n",
    "    children = []\n",
    "\n",
    "    # Add Heading\n",
    "    if \"heading\" in content_dict and content_dict[\"heading\"]:\n",
    "        children.append(\n",
    "            {\n",
    "                \"object\": \"block\",\n",
    "                \"type\": \"heading_2\",\n",
    "                \"heading_2\": {\n",
    "                    \"rich_text\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": {\n",
    "                                \"content\": content_dict[\"heading\"]\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Add Content\n",
    "    if \"content\" in content_dict and content_dict[\"content\"]:\n",
    "        children.append(\n",
    "            {\n",
    "                \"object\": \"block\",\n",
    "                \"type\": \"paragraph\",\n",
    "                \"paragraph\": {\n",
    "                    \"rich_text\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": {\n",
    "                                \"content\": content_dict[\"content\"]\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Add List Items (Bullet Points)\n",
    "    if \"list\" in content_dict and content_dict[\"list\"]:\n",
    "        list_blocks = build_bullet_list(content_dict[\"list\"])\n",
    "        children.extend(list_blocks)\n",
    "        \n",
    "    # Add URL as a Link\n",
    "    if \"url\" in content_dict and content_dict[\"url\"]:\n",
    "        children.append(\n",
    "            {\n",
    "                \"object\": \"block\",\n",
    "                \"type\": \"paragraph\",\n",
    "                \"paragraph\": {\n",
    "                    \"rich_text\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": {\n",
    "                                \"content\": content_dict[\"url\"],\n",
    "                                \"link\": {\"url\": content_dict[\"url\"]}\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    return children\n",
    "\n",
    "\n",
    "def build_bullet_list(items):\n",
    "    \"\"\"\n",
    "    Builds Notion bullet list blocks from a list of items.\n",
    "\n",
    "    Args:\n",
    "        items (list): A list of strings representing bullet points.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Notion bulleted list item block objects.\n",
    "    \"\"\"\n",
    "    bullet_blocks = []\n",
    "    for item in items:\n",
    "        bullet_blocks.append(\n",
    "            {\n",
    "                \"object\": \"block\",\n",
    "                \"type\": \"bulleted_list_item\",\n",
    "                \"bulleted_list_item\": {\n",
    "                    \"rich_text\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": {\n",
    "                                \"content\": item\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "    return bullet_blocks\n",
    "\n",
    "\n",
    "def build_children_from_sections(content_sections):\n",
    "    \"\"\"\n",
    "    Iterates through the content sections dictionary and builds the children blocks.\n",
    "\n",
    "    Args:\n",
    "        content_sections (dict): Dictionary containing all content sections.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Notion block objects.\n",
    "    \"\"\"\n",
    "    children = []\n",
    "    for key in sorted(content_sections.keys()):\n",
    "        section = content_sections[key]\n",
    "        section_blocks = build_content_from_dict(section)\n",
    "        children.extend(section_blocks)\n",
    "    return children\n",
    "\n",
    "\n",
    "def handle_page_creation_or_append(title, database_id, content_sections):\n",
    "    \"\"\"\n",
    "    Handles the logic to either create a new page or append content to an existing page.\n",
    "\n",
    "    Args:\n",
    "        title (str): The title of the page.\n",
    "        database_id (str): The ID of the Notion database.\n",
    "        content_sections (dict): Dictionary containing all content sections.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    full_title = f\"{title} - {current_date}\"\n",
    "\n",
    "    # Build the content blocks\n",
    "    children = build_children_from_sections(content_sections)\n",
    "\n",
    "    # Check if the page already exists\n",
    "    existing_page = find_page_by_title(database_id, full_title)\n",
    "\n",
    "    if existing_page:\n",
    "        print(f\"Page '{full_title}' already exists. Appending new content to it.\")\n",
    "        page_id = existing_page[\"id\"]\n",
    "        response = append_to_page(page_id, children)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"New content appended successfully.\")\n",
    "            # Construct the page URL manually\n",
    "            # Note: Notion page URLs follow the format https://www.notion.so/{workspace}/{page_id}\n",
    "            # However, constructing the exact URL might require additional steps.\n",
    "            # Here, we'll provide a placeholder.\n",
    "            page_url = f\"https://www.notion.so/{page_id.replace('-', '')}\"\n",
    "            print(f\"View your page here: {page_url}\")\n",
    "        else:\n",
    "            print(\"Failed to append new content:\")\n",
    "            print(json.dumps(response.json(), indent=2))\n",
    "    else:\n",
    "        print(f\"Page '{full_title}' does not exist. Creating a new page with the new content.\")\n",
    "        response = create_page(full_title, database_id, children)\n",
    "        \n",
    "        # Handle the response\n",
    "        if response.status_code == 200:\n",
    "            page_url = response.json().get(\"url\", \"No URL returned\")\n",
    "            print(\"Page created successfully with the new content.\")\n",
    "            print(f\"View your page here: {page_url}\")\n",
    "        else:\n",
    "            print(\"Failed to create page:\")\n",
    "            print(json.dumps(response.json(), indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "523f7939-7050-4a27-bdb3-599c2cd6cb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/lobbying/public\n",
      "Requesting URL: https://data.sov.ai/lobbying/public with params: {'parquet': 'True', 'full_history': 'True'}\n",
      "Response Status: 200\n",
      "Response Content-Type: application/json\n",
      "tickers is None\n",
      "{'parquet': 'True', 'full_history': 'True'}\n",
      "tickers is: \n",
      "All ticker Initialized\n",
      "Attempting URL 1: https://s3.wasabisys.com/sovai-lobbying/data/lobbying_all_years_public.parquet?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=KI07G4DK9XP5EC0NH1VU%2F20241030%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241030T102328Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=329f5839fe4a27d578bd52a3283e4fd3cffdb502e80587ff831c61010193bdd4 (download link)\n",
      "Successfully downloaded data from URL 1\n",
      "It reached the DF\n",
      "It passed the DF\n"
     ]
    }
   ],
   "source": [
    "import sovai as sov\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "sov.token_auth(token=\"visit https://sov.ai/profile for your token\")\n",
    "\n",
    "tickers_meta = pd.read_parquet(\"data/tickers.parq\")\n",
    "\n",
    "df_lobbying = sov.data(\"lobbying/public\",verbose=True, full_history=True)\n",
    "\n",
    "df_lobbying = df_lobbying.reset_index()\n",
    "\n",
    "\n",
    "# Optional: For Time Series Anomaly Detection\n",
    "# Uncomment the following lines if you plan to use Prophet\n",
    "# !pip install prophet\n",
    "# from prophet import Prophet\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 1: Load and Preprocess Data\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Load your DataFrame\n",
    "# Replace 'your_data.csv' with your actual data source\n",
    "# Example:\n",
    "# df_lobbying = pd.read_csv('your_data.csv')\n",
    "\n",
    "# For demonstration purposes, let's assume df_lobbying is already loaded\n",
    "# Uncomment and modify the following line as needed:\n",
    "# df_lobbying = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Convert 'date' column to datetime\n",
    "df_lobbying['date'] = pd.to_datetime(df_lobbying['date'], errors='coerce')\n",
    "\n",
    "# Drop rows with invalid dates\n",
    "df_lobbying.dropna(subset=['date'], inplace=True)\n",
    "\n",
    "# Determine the maximum date in the dataset\n",
    "max_date = df_lobbying['date'].max()\n",
    "\n",
    "# Calculate the cutoff date (3 years before max_date)\n",
    "three_years = pd.DateOffset(years=3)\n",
    "cutoff_date = max_date - three_years\n",
    "\n",
    "# Filter data for the last three years\n",
    "recent_data = df_lobbying[df_lobbying['date'] >= cutoff_date].copy()\n",
    "\n",
    "# Reset index if necessary\n",
    "recent_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 2: Handle List-like Columns\n",
    "# ---------------------------------------------\n",
    "\n",
    "def safe_convert(x):\n",
    "    \"\"\"\n",
    "    Safely convert string representations of lists to actual lists.\n",
    "    Uses ast.literal_eval for security.\n",
    "    \"\"\"\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return []\n",
    "    elif isinstance(x, (list, np.ndarray, tuple)):\n",
    "        return list(x)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# List of columns that contain list-like data\n",
    "list_like_columns = [\n",
    "    'government_entity_details',\n",
    "    'issue_codes',\n",
    "    'previous_goverment_positions',\n",
    "    'lobbyist_new_statuses',\n",
    "    'lobbyist_full_names',\n",
    "    'lobbyist_ids',\n",
    "    'registrant_contact_name',\n",
    "    'registrant_house_registrant_id',\n",
    "    'registrant_contact_telephone',\n",
    "    'match'\n",
    "]\n",
    "\n",
    "# Apply the conversion to list-like columns\n",
    "for col in list_like_columns:\n",
    "    if col in recent_data.columns:\n",
    "        recent_data[col] = recent_data[col].apply(safe_convert)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 3: Weekly Aggregation Per Ticker\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Define aggregation functions for scalar columns\n",
    "scalar_aggregations = {\n",
    "    'spend': ['sum', 'mean', 'median'],\n",
    "    'transaction_type': 'count',\n",
    "    'client': 'nunique'\n",
    "    # 'government_entity_details': 'nunique'  # Removed due to list-like entries\n",
    "}\n",
    "\n",
    "# Group by 'ticker' and resample weekly\n",
    "weekly_scalar = recent_data.groupby(\n",
    "    ['ticker', pd.Grouper(key='date', freq='W')]\n",
    ").agg(scalar_aggregations)\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "weekly_scalar.columns = ['_'.join(col).strip() for col in weekly_scalar.columns.values]\n",
    "\n",
    "# Handle 'government_entity_details' separately\n",
    "def count_unique_government_entities(series):\n",
    "    unique_entities = set()\n",
    "    for entry in series:\n",
    "        if isinstance(entry, (list, tuple, np.ndarray)):\n",
    "            unique_entities.update(entry)\n",
    "        elif pd.notnull(entry):\n",
    "            unique_entities.add(entry)\n",
    "    return len(unique_entities)\n",
    "\n",
    "weekly_gov_entities = recent_data.groupby(\n",
    "    ['ticker', pd.Grouper(key='date', freq='W')]\n",
    ")['government_entity_details'].apply(count_unique_government_entities).rename('Unique_Government_Entities')\n",
    "\n",
    "# Group by 'ticker' and week, then aggregate spend by transaction type\n",
    "spend_by_type = recent_data.groupby(\n",
    "    ['ticker', pd.Grouper(key='date', freq='W'), 'transaction_type']\n",
    ")['spend'].sum().unstack(fill_value=0).rename(columns=lambda x: f\"Spend_by_{x}\")\n",
    "\n",
    "# Combine scalar aggregations with unique government entities\n",
    "weekly_stats = weekly_scalar.join(weekly_gov_entities)\n",
    "\n",
    "# Combine with spend by transaction type\n",
    "weekly_stats = weekly_stats.join(spend_by_type)\n",
    "\n",
    "# Reset index to turn 'ticker' and 'date' into columns\n",
    "weekly_stats_reset = weekly_stats.reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "weekly_stats_reset.rename(columns={\n",
    "    'spend_sum': 'Total_Spend',\n",
    "    'spend_mean': 'Average_Spend',\n",
    "    'spend_median': 'Median_Spend',\n",
    "    'transaction_type_count': 'Transaction_Count',\n",
    "    'client_nunique': 'Unique_Clients'\n",
    "    # 'Unique_Government_Entities' is already appropriately named\n",
    "}, inplace=True)\n",
    "\n",
    "# Handle missing values (if any)\n",
    "weekly_stats_reset.fillna(0, inplace=True)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 4: Anomaly Detection Methods\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Group data by 'ticker' for calculations\n",
    "grouped = weekly_stats_reset.groupby('ticker')\n",
    "\n",
    "# --- Method 1: Z-Score ---\n",
    "weekly_stats_reset['Mean_Spend'] = grouped['Total_Spend'].transform('mean')\n",
    "weekly_stats_reset['Std_Spend'] = grouped['Total_Spend'].transform('std')\n",
    "weekly_stats_reset['Z_Score'] = (weekly_stats_reset['Total_Spend'] - weekly_stats_reset['Mean_Spend']) / weekly_stats_reset['Std_Spend']\n",
    "weekly_stats_reset['Z_Score'].fillna(0, inplace=True)  # Handle NaN std (e.g., std=0)\n",
    "\n",
    "# Define Z-Score threshold\n",
    "threshold_z = 2\n",
    "\n",
    "# Flag anomalies based on Z-Score\n",
    "weekly_stats_reset['Anomaly_Z_Score'] = weekly_stats_reset['Z_Score'] > threshold_z\n",
    "\n",
    "# --- Method 2: Moving Average and Standard Deviation ---\n",
    "window_size = 4  # e.g., 4 weeks\n",
    "n_std = 2  # Number of standard deviations\n",
    "\n",
    "# Sort data by 'ticker' and 'date' to ensure correct rolling calculations\n",
    "weekly_stats_reset.sort_values(['ticker', 'date'], inplace=True)\n",
    "\n",
    "# Calculate rolling mean and std\n",
    "weekly_stats_reset['Rolling_Mean'] = grouped['Total_Spend'].transform(lambda x: x.rolling(window=window_size, min_periods=1).mean())\n",
    "weekly_stats_reset['Rolling_Std'] = grouped['Total_Spend'].transform(lambda x: x.rolling(window=window_size, min_periods=1).std())\n",
    "weekly_stats_reset['Rolling_Std'].fillna(0, inplace=True)  # Handle NaN std\n",
    "\n",
    "# Calculate excess spend over the threshold\n",
    "weekly_stats_reset['Excess_Spend_Moving_Avg'] = weekly_stats_reset['Total_Spend'] - (weekly_stats_reset['Rolling_Mean'] + n_std * weekly_stats_reset['Rolling_Std'])\n",
    "\n",
    "# Replace negative excess spends with 0 (only consider positive deviations)\n",
    "weekly_stats_reset['Excess_Spend_Moving_Avg'] = weekly_stats_reset['Excess_Spend_Moving_Avg'].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "# Flag anomalies based on Moving Average\n",
    "weekly_stats_reset['Anomaly_Moving_Avg'] = weekly_stats_reset['Excess_Spend_Moving_Avg'] > 0\n",
    "\n",
    "# --- Method 3: Percentile-Based ---\n",
    "percentile = 0.90  # 90th percentile\n",
    "\n",
    "# Calculate the 90th percentile for each ticker\n",
    "weekly_stats_reset['Spend_90th_Percentile'] = grouped['Total_Spend'].transform(lambda x: x.quantile(percentile))\n",
    "\n",
    "# Calculate excess spend over the percentile\n",
    "weekly_stats_reset['Excess_Spend_Percentile'] = weekly_stats_reset['Total_Spend'] - weekly_stats_reset['Spend_90th_Percentile']\n",
    "\n",
    "# Replace negative excess spends with 0 (only consider positive deviations)\n",
    "weekly_stats_reset['Excess_Spend_Percentile'] = weekly_stats_reset['Excess_Spend_Percentile'].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "# Flag anomalies based on Percentile\n",
    "weekly_stats_reset['Anomaly_Percentile'] = weekly_stats_reset['Excess_Spend_Percentile'] > 0\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 5: Continuous Anomaly Scoring\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Compute Percentile Ranks for each anomaly detection method within each ticker\n",
    "\n",
    "# --- Percentile Rank for Z-Score ---\n",
    "weekly_stats_reset['Z_Score_Pct'] = grouped['Z_Score'].transform(lambda x: x.rank(pct=True))\n",
    "\n",
    "# --- Percentile Rank for Moving Average Excess Spend ---\n",
    "weekly_stats_reset['Excess_Spend_Moving_Avg_Pct'] = grouped['Excess_Spend_Moving_Avg'].transform(lambda x: x.rank(pct=True))\n",
    "\n",
    "# --- Percentile Rank for Percentile-Based Excess Spend ---\n",
    "weekly_stats_reset['Excess_Spend_Percentile_Pct'] = grouped['Excess_Spend_Percentile'].transform(lambda x: x.rank(pct=True))\n",
    "\n",
    "\n",
    "# Calculate the mean of the percentile ranks to get a consolidated anomaly score\n",
    "weekly_stats_reset['Anomaly_Score'] = (\n",
    "    weekly_stats_reset['Z_Score_Pct'] +\n",
    "    weekly_stats_reset['Excess_Spend_Moving_Avg_Pct'] +\n",
    "    weekly_stats_reset['Excess_Spend_Percentile_Pct']\n",
    "    # + weekly_stats_reset['Anomaly_Prophet_Pct']  # Uncomment if using Prophet\n",
    ") / 3  # Adjust denominator based on the number of methods used\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 1: Load and Preprocess Data\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Load your DataFrame\n",
    "# Replace 'your_data.csv' with your actual data source\n",
    "# Example:\n",
    "# df_lobbying = pd.read_csv('your_data.csv')\n",
    "\n",
    "# For demonstration purposes, let's assume df_lobbying is already loaded\n",
    "# Uncomment and modify the following line as needed:\n",
    "# df_lobbying = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Ensure 'df_lobbying' is loaded. If not, raise an error.\n",
    "try:\n",
    "    df_lobbying\n",
    "except NameError:\n",
    "    raise NameError(\"DataFrame 'df_lobbying' is not loaded. Please load your data before proceeding.\")\n",
    "\n",
    "# Convert 'date' column to datetime\n",
    "df_lobbying['date'] = pd.to_datetime(df_lobbying['date'], errors='coerce')\n",
    "\n",
    "# Drop rows with invalid dates\n",
    "df_lobbying.dropna(subset=['date'], inplace=True)\n",
    "\n",
    "# Determine the maximum date in the dataset\n",
    "max_date = df_lobbying['date'].max()\n",
    "\n",
    "# Calculate the cutoff date (3 years before max_date)\n",
    "three_years = pd.DateOffset(years=3)\n",
    "cutoff_date = max_date - three_years\n",
    "\n",
    "# Compute Total Spend over the entire dataset per ticker\n",
    "total_spend_all = df_lobbying.groupby('ticker')['spend'].sum().reset_index().rename(columns={'spend': 'Total_Spend_All'})\n",
    "\n",
    "# Filter data for the last three years\n",
    "recent_data = df_lobbying[df_lobbying['date'] >= cutoff_date].copy()\n",
    "\n",
    "# Reset index if necessary\n",
    "recent_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 2: Handle List-like Columns\n",
    "# ---------------------------------------------\n",
    "\n",
    "def safe_convert(x):\n",
    "    \"\"\"\n",
    "    Safely convert string representations of lists to actual lists.\n",
    "    Uses ast.literal_eval for security.\n",
    "    \"\"\"\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return []\n",
    "    elif isinstance(x, (list, np.ndarray, tuple)):\n",
    "        return list(x)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# List of columns that contain list-like data\n",
    "list_like_columns = [\n",
    "    'government_entity_details',\n",
    "    'issue_codes',\n",
    "    'previous_goverment_positions',\n",
    "    'lobbyist_new_statuses',\n",
    "    'lobbyist_full_names',\n",
    "    'lobbyist_ids',\n",
    "    'registrant_contact_name',\n",
    "    'registrant_house_registrant_id',\n",
    "    'registrant_contact_telephone',\n",
    "    'match'\n",
    "]\n",
    "\n",
    "# Apply the conversion to list-like columns\n",
    "for col in list_like_columns:\n",
    "    if col in recent_data.columns:\n",
    "        recent_data[col] = recent_data[col].apply(safe_convert)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 3: Weekly Aggregation Per Ticker\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Define aggregation functions for scalar columns\n",
    "scalar_aggregations = {\n",
    "    'spend': ['sum', 'mean', 'median'],\n",
    "    'transaction_type': 'count',\n",
    "    'client': 'nunique'\n",
    "    # 'government_entity_details': 'nunique'  # Removed due to list-like entries\n",
    "}\n",
    "\n",
    "# Group by 'ticker' and resample weekly\n",
    "weekly_scalar = recent_data.groupby(\n",
    "    ['ticker', pd.Grouper(key='date', freq='W')]\n",
    ").agg(scalar_aggregations)\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "weekly_scalar.columns = ['_'.join(col).strip() for col in weekly_scalar.columns.values]\n",
    "\n",
    "# Handle 'government_entity_details' separately\n",
    "def count_unique_government_entities(series):\n",
    "    unique_entities = set()\n",
    "    for entry in series:\n",
    "        if isinstance(entry, (list, tuple, np.ndarray)):\n",
    "            unique_entities.update(entry)\n",
    "        elif pd.notnull(entry):\n",
    "            unique_entities.add(entry)\n",
    "    return len(unique_entities)\n",
    "\n",
    "weekly_gov_entities = recent_data.groupby(\n",
    "    ['ticker', pd.Grouper(key='date', freq='W')]\n",
    ")['government_entity_details'].apply(count_unique_government_entities).rename('Unique_Government_Entities')\n",
    "\n",
    "# Handle 'Unique_Lobbyists' by counting unique lobbyist_ids\n",
    "def count_unique_lobbyists(series):\n",
    "    unique_lobbyists = set()\n",
    "    for entry in series:\n",
    "        if isinstance(entry, (list, tuple, np.ndarray)):\n",
    "            unique_lobbyists.update(entry)\n",
    "        elif pd.notnull(entry):\n",
    "            unique_lobbyists.add(entry)\n",
    "    return len(unique_lobbyists)\n",
    "\n",
    "weekly_unique_lobbyists = recent_data.groupby(\n",
    "    ['ticker', pd.Grouper(key='date', freq='W')]\n",
    ")['lobbyist_ids'].apply(count_unique_lobbyists).rename('Unique_Lobbyists')\n",
    "\n",
    "# Group by 'ticker' and week, then aggregate spend by transaction type\n",
    "spend_by_type = recent_data.groupby(\n",
    "    ['ticker', pd.Grouper(key='date', freq='W'), 'transaction_type']\n",
    ")['spend'].sum().unstack(fill_value=0).rename(columns=lambda x: f\"Spend_by_{x}\")\n",
    "\n",
    "# Combine scalar aggregations with unique government entities and unique lobbyists\n",
    "weekly_stats = weekly_scalar.join([weekly_gov_entities, weekly_unique_lobbyists])\n",
    "\n",
    "# Combine with spend by transaction type\n",
    "weekly_stats = weekly_stats.join(spend_by_type)\n",
    "\n",
    "# Reset index to turn 'ticker' and 'date' into columns\n",
    "weekly_stats_reset = weekly_stats.reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "weekly_stats_reset.rename(columns={\n",
    "    'spend_sum': 'Total_Spend',\n",
    "    'spend_mean': 'Average_Spend',\n",
    "    'spend_median': 'Median_Spend',\n",
    "    'transaction_type_count': 'Transaction_Count',\n",
    "    'client_nunique': 'Unique_Clients'\n",
    "    # 'Unique_Government_Entities' and 'Unique_Lobbyists' are already appropriately named\n",
    "}, inplace=True)\n",
    "\n",
    "# Handle missing values (if any)\n",
    "weekly_stats_reset.fillna(0, inplace=True)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 4: Anomaly Detection Methods\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Group data by 'ticker' for calculations\n",
    "grouped = weekly_stats_reset.groupby('ticker')\n",
    "\n",
    "# --- Method 1: Z-Score ---\n",
    "weekly_stats_reset['Mean_Spend'] = grouped['Total_Spend'].transform('mean')\n",
    "weekly_stats_reset['Std_Spend'] = grouped['Total_Spend'].transform('std')\n",
    "weekly_stats_reset['Z_Score'] = (weekly_stats_reset['Total_Spend'] - weekly_stats_reset['Mean_Spend']) / weekly_stats_reset['Std_Spend']\n",
    "weekly_stats_reset['Z_Score'].fillna(0, inplace=True)  # Handle NaN std (e.g., std=0)\n",
    "\n",
    "# --- Method 2: Moving Average and Standard Deviation ---\n",
    "window_size = 4  # e.g., 4 weeks\n",
    "n_std = 2  # Number of standard deviations\n",
    "\n",
    "# Sort data by 'ticker' and 'date' to ensure correct rolling calculations\n",
    "weekly_stats_reset.sort_values(['ticker', 'date'], inplace=True)\n",
    "\n",
    "# Calculate rolling mean and std\n",
    "weekly_stats_reset['Rolling_Mean'] = grouped['Total_Spend'].transform(lambda x: x.rolling(window=window_size, min_periods=1).mean())\n",
    "weekly_stats_reset['Rolling_Std'] = grouped['Total_Spend'].transform(lambda x: x.rolling(window=window_size, min_periods=1).std())\n",
    "weekly_stats_reset['Rolling_Std'].fillna(0, inplace=True)  # Handle NaN std\n",
    "\n",
    "# Calculate excess spend over the threshold\n",
    "weekly_stats_reset['Excess_Spend_Moving_Avg'] = weekly_stats_reset['Total_Spend'] - (weekly_stats_reset['Rolling_Mean'] + n_std * weekly_stats_reset['Rolling_Std'])\n",
    "\n",
    "# Replace negative excess spends with 0 (only consider positive deviations)\n",
    "weekly_stats_reset['Excess_Spend_Moving_Avg'] = weekly_stats_reset['Excess_Spend_Moving_Avg'].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "# Flag anomalies based on Moving Average\n",
    "weekly_stats_reset['Anomaly_Moving_Avg'] = weekly_stats_reset['Excess_Spend_Moving_Avg'] > 0\n",
    "\n",
    "# --- Method 3: Percentile-Based ---\n",
    "percentile = 0.90  # 90th percentile\n",
    "\n",
    "# Calculate the 90th percentile for each ticker\n",
    "weekly_stats_reset['Spend_90th_Percentile'] = grouped['Total_Spend'].transform(lambda x: x.quantile(percentile))\n",
    "\n",
    "# Calculate excess spend over the percentile\n",
    "weekly_stats_reset['Excess_Spend_Percentile'] = weekly_stats_reset['Total_Spend'] - weekly_stats_reset['Spend_90th_Percentile']\n",
    "\n",
    "# Replace negative excess spends with 0 (only consider positive deviations)\n",
    "weekly_stats_reset['Excess_Spend_Percentile'] = weekly_stats_reset['Excess_Spend_Percentile'].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "# Flag anomalies based on Percentile\n",
    "weekly_stats_reset['Anomaly_Percentile'] = weekly_stats_reset['Excess_Spend_Percentile'] > 0\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 5: Continuous Anomaly Scoring\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Compute Percentile Ranks for each anomaly detection method within each ticker\n",
    "\n",
    "# --- Percentile Rank for Z-Score ---\n",
    "weekly_stats_reset['Z_Score_Pct'] = grouped['Z_Score'].transform(lambda x: x.rank(pct=True))\n",
    "\n",
    "# --- Percentile Rank for Moving Average Excess Spend ---\n",
    "weekly_stats_reset['Excess_Spend_Moving_Avg_Pct'] = grouped['Excess_Spend_Moving_Avg'].transform(lambda x: x.rank(pct=True))\n",
    "\n",
    "# --- Percentile Rank for Percentile-Based Excess Spend ---\n",
    "weekly_stats_reset['Excess_Spend_Percentile_Pct'] = grouped['Excess_Spend_Percentile'].transform(lambda x: x.rank(pct=True))\n",
    "\n",
    "# --- Percentile Rank for Total Spend ---\n",
    "# Incorporate Total_Spend as an additional factor\n",
    "weekly_stats_reset['Total_Spend_Pct'] = grouped['Total_Spend'].transform(lambda x: x.rank(pct=True))\n",
    "\n",
    "# Calculate the mean of the percentile ranks to get a consolidated anomaly score\n",
    "# Now, including Total_Spend_Pct, so denominator becomes 4\n",
    "weekly_stats_reset['Anomaly_Score'] = (\n",
    "    weekly_stats_reset['Z_Score_Pct'] +\n",
    "    weekly_stats_reset['Excess_Spend_Moving_Avg_Pct'] +\n",
    "    weekly_stats_reset['Excess_Spend_Percentile_Pct'] +\n",
    "    weekly_stats_reset['Total_Spend_Pct']\n",
    ") / 4  # Adjust denominator based on the number of methods used\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 6: Flagging Anomalies Based on Anomaly_Score\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Define a threshold for anomaly score (e.g., top 95th percentile)\n",
    "overall_threshold = weekly_stats_reset['Anomaly_Score'].quantile(0.95)\n",
    "\n",
    "# Flag anomalies where Anomaly_Score exceeds the threshold\n",
    "weekly_stats_reset['Anomaly_Flag'] = weekly_stats_reset['Anomaly_Score'] > overall_threshold\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 7: Compute Quarterly and Annual Aggregates\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Add 'year' and 'quarter' columns\n",
    "weekly_stats_reset['year'] = weekly_stats_reset['date'].dt.year\n",
    "weekly_stats_reset['quarter'] = weekly_stats_reset['date'].dt.to_period('Q')\n",
    "\n",
    "# --- Quarterly Aggregates ---\n",
    "quarterly_aggregates = weekly_stats_reset.groupby(['ticker', 'quarter']).agg({\n",
    "    'Total_Spend': ['sum', 'mean', 'median'],\n",
    "    'Transaction_Count': 'sum',\n",
    "    'Unique_Clients': 'sum',  # Assuming sum makes sense; else use 'nunique'\n",
    "    'Unique_Government_Entities': 'sum'  # Assuming sum makes sense\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "quarterly_aggregates.columns = ['ticker', 'quarter',\n",
    "                                'Q_TS',  # Quarterly_Total_Spend\n",
    "                                'Q_AS',  # Quarterly_Average_Spend\n",
    "                                'Q_MS',  # Quarterly_Median_Spend\n",
    "                                'Q_TC',  # Quarterly_Transaction_Count\n",
    "                                'Q_UC',  # Quarterly_Unique_Clients\n",
    "                                'Q_UGE']  # Quarterly_Unique_Government_Entities\n",
    "\n",
    "# --- Annual Aggregates ---\n",
    "annual_aggregates = weekly_stats_reset.groupby(['ticker', 'year']).agg({\n",
    "    'Total_Spend': ['sum', 'mean', 'median'],\n",
    "    'Transaction_Count': 'sum',\n",
    "    'Unique_Clients': 'sum',  # Assuming sum makes sense; else use 'nunique'\n",
    "    'Unique_Government_Entities': 'sum'  # Assuming sum makes sense\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "annual_aggregates.columns = ['ticker', 'year',\n",
    "                             'A_TS',  # Annual_Total_Spend\n",
    "                             'A_AS',  # Annual_Average_Spend\n",
    "                             'A_MS',  # Annual_Median_Spend\n",
    "                             'A_TC',  # Annual_Transaction_Count\n",
    "                             'A_UC',  # Annual_Unique_Clients\n",
    "                             'A_UGE']  # Annual_Unique_Government_Entities\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 8: Merge Quarterly and Annual Aggregates with Weekly Data\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Merge quarterly aggregates\n",
    "weekly_stats_reset = weekly_stats_reset.merge(\n",
    "    quarterly_aggregates,\n",
    "    on=['ticker', 'quarter'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge annual aggregates\n",
    "weekly_stats_reset = weekly_stats_reset.merge(\n",
    "    annual_aggregates,\n",
    "    on=['ticker', 'year'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge Total_Spend_All into weekly_stats_reset\n",
    "weekly_stats_reset = weekly_stats_reset.merge(\n",
    "    total_spend_all,\n",
    "    on='ticker',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 9: Select Only the Latest Week's Data\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Identify the latest week date in the dataset\n",
    "latest_week_date = weekly_stats_reset['date'].max()\n",
    "\n",
    "# Filter the DataFrame to include only the latest week's data\n",
    "latest_week_data = weekly_stats_reset[weekly_stats_reset['date'] == latest_week_date].copy()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 10: Clean Up and Retain Only Relevant Columns with Descriptive Names\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Define the columns to retain with more descriptive and self-explanatory names\n",
    "# Selected 10 key columns:\n",
    "# 1. Ticker\n",
    "# 2. Date\n",
    "# 3. Anomaly_Score\n",
    "# 4. Total_Spend (Week Spend)\n",
    "# 5. Quarterly_Spend (Quarter Spend)\n",
    "# 6. Annual_Spend (Year Spend)\n",
    "# 7. Total_Spend_All (Total Spend)\n",
    "# 8. Transaction_Count (Transactions)\n",
    "# 9. Unique_Lobbyists (Lobbyist)\n",
    "# 10. Unique_Government_Entities (Gov Entities)\n",
    "\n",
    "columns_to_keep = [\n",
    "    'ticker',                      # Ticker\n",
    "    'date',                        # Date\n",
    "    'Anomaly_Score',               # Anomaly\n",
    "    'Total_Spend',                 # Week Spend\n",
    "    'Q_TS',                        # Quarter Spend\n",
    "    'A_TS',                        # Year Spend\n",
    "    'Total_Spend_All',             # Total Spend\n",
    "    'Transaction_Count',           # Transactions\n",
    "    'Unique_Lobbyists',            # Lobbyist\n",
    "    'Unique_Government_Entities'   # Gov Entities\n",
    "]\n",
    "\n",
    "# Verify which of these columns exist in the DataFrame\n",
    "existing_columns = [col for col in columns_to_keep if col in latest_week_data.columns]\n",
    "\n",
    "# Create a mapping for shorter and more descriptive names\n",
    "column_mapping = {\n",
    "    'ticker': 'Ticker',\n",
    "    'date': 'Date',\n",
    "    'Anomaly_Score': 'Anomaly',\n",
    "    'Total_Spend': 'Week Spend',\n",
    "    'Q_TS': 'Quarter Spend',\n",
    "    'A_TS': 'Year Spend',\n",
    "    'Total_Spend_All': 'Total Spend',\n",
    "    'Transaction_Count': 'Transaction Counts',\n",
    "    'Unique_Lobbyists': 'Lobbyist Counts',\n",
    "    'Unique_Government_Entities': 'Gov Entities'\n",
    "}\n",
    "\n",
    "# Filter the DataFrame\n",
    "final_output = latest_week_data[existing_columns].copy()\n",
    "\n",
    "# Rename the columns to more descriptive names\n",
    "final_output.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 11: Scale Spending Columns by 1000\n",
    "# ---------------------------------------------\n",
    "\n",
    "# List of spending columns to scale\n",
    "spend_columns = ['Week Spend', 'Quarter Spend', 'Year Spend', 'Total Spend']\n",
    "\n",
    "# Check if these columns exist before scaling\n",
    "spend_columns_existing = [col for col in spend_columns if col in final_output.columns]\n",
    "\n",
    "# Scale the spending columns by dividing by 1000\n",
    "final_output[spend_columns_existing] = final_output[spend_columns_existing] / 1000\n",
    "\n",
    "# Optionally, round the spending columns for better readability\n",
    "final_output[spend_columns_existing] = final_output[spend_columns_existing].round(3)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 12: Final Output\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Define the final column order as specified\n",
    "final_output = final_output[[\n",
    "    \"Ticker\", \n",
    "    \"Date\",\n",
    "    \"Anomaly\", \n",
    "    \"Week Spend\",\n",
    "    \"Quarter Spend\", \n",
    "    \"Year Spend\", \n",
    "    \"Total Spend\", \n",
    "    \"Transaction Counts\",\n",
    "    \"Lobbyist Counts\",\n",
    "    \"Gov Entities\"\n",
    "]]\n",
    "\n",
    "# Sort the final output by Anoma\n",
    "\n",
    "\n",
    "final_output.shape\n",
    "\n",
    "final_output = final_output.sort_values(\"Anomaly\",ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "# Create Yahoo Finance links for tickers\n",
    "final_output['Ticker'] = final_output['Ticker'].apply(\n",
    "    lambda x: f\"[{x}](https://finance.yahoo.com/quote/{x})\"\n",
    ")\n",
    "\n",
    "# Remove the Date column\n",
    "final_output = final_output.drop(columns=['Date'])\n",
    "\n",
    "\n",
    "# Configure visualization\n",
    "# Configure visualization\n",
    "# First rename the column in your DataFrame\n",
    "final_output = final_output.rename(columns={'Transaction Counts': 'Trans. Counts',\"Lobbyist Counts\":'Lobby Accounts'})\n",
    "\n",
    "\n",
    "final_output = final_output.drop(columns=[\"Trans. Counts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5669d4a-f801-4e4b-ae63-b75e1f5f262f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Anomaly</th>\n",
       "      <th>Week Spend</th>\n",
       "      <th>Quarter Spend</th>\n",
       "      <th>Year Spend</th>\n",
       "      <th>Total Spend</th>\n",
       "      <th>Lobby Accounts</th>\n",
       "      <th>Gov Entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9573</th>\n",
       "      <td>[IDXG](https://finance.yahoo.com/quote/IDXG)</td>\n",
       "      <td>1.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>320.000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5394</th>\n",
       "      <td>[CUSI](https://finance.yahoo.com/quote/CUSI)</td>\n",
       "      <td>1.000</td>\n",
       "      <td>40.000</td>\n",
       "      <td>40.000</td>\n",
       "      <td>40.000</td>\n",
       "      <td>40.000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12404</th>\n",
       "      <td>[NEOV](https://finance.yahoo.com/quote/NEOV)</td>\n",
       "      <td>1.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9059</th>\n",
       "      <td>[HNSBF](https://finance.yahoo.com/quote/HNSBF)</td>\n",
       "      <td>1.000</td>\n",
       "      <td>60.000</td>\n",
       "      <td>60.000</td>\n",
       "      <td>60.000</td>\n",
       "      <td>60.000</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13873</th>\n",
       "      <td>[PATH](https://finance.yahoo.com/quote/PATH)</td>\n",
       "      <td>0.938</td>\n",
       "      <td>60.000</td>\n",
       "      <td>60.000</td>\n",
       "      <td>110.000</td>\n",
       "      <td>110.000</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5036</th>\n",
       "      <td>[CPNG](https://finance.yahoo.com/quote/CPNG)</td>\n",
       "      <td>0.286</td>\n",
       "      <td>90.000</td>\n",
       "      <td>880.000</td>\n",
       "      <td>3310.000</td>\n",
       "      <td>6740.000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>[AON](https://finance.yahoo.com/quote/AON)</td>\n",
       "      <td>0.285</td>\n",
       "      <td>20.000</td>\n",
       "      <td>70.000</td>\n",
       "      <td>470.000</td>\n",
       "      <td>6395.000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18248</th>\n",
       "      <td>[USB](https://finance.yahoo.com/quote/USB)</td>\n",
       "      <td>0.281</td>\n",
       "      <td>5.000</td>\n",
       "      <td>385.000</td>\n",
       "      <td>1555.000</td>\n",
       "      <td>25678.037</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15587</th>\n",
       "      <td>[ROKU](https://finance.yahoo.com/quote/ROKU)</td>\n",
       "      <td>0.280</td>\n",
       "      <td>30.000</td>\n",
       "      <td>290.000</td>\n",
       "      <td>1140.000</td>\n",
       "      <td>3090.000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3884</th>\n",
       "      <td>[CAH](https://finance.yahoo.com/quote/CAH)</td>\n",
       "      <td>0.278</td>\n",
       "      <td>10.000</td>\n",
       "      <td>254.000</td>\n",
       "      <td>1956.000</td>\n",
       "      <td>40545.000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>677 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Ticker  Anomaly  Week Spend  \\\n",
       "9573     [IDXG](https://finance.yahoo.com/quote/IDXG)    1.000      30.000   \n",
       "5394     [CUSI](https://finance.yahoo.com/quote/CUSI)    1.000      40.000   \n",
       "12404    [NEOV](https://finance.yahoo.com/quote/NEOV)    1.000      20.000   \n",
       "9059   [HNSBF](https://finance.yahoo.com/quote/HNSBF)    1.000      60.000   \n",
       "13873    [PATH](https://finance.yahoo.com/quote/PATH)    0.938      60.000   \n",
       "...                                               ...      ...         ...   \n",
       "5036     [CPNG](https://finance.yahoo.com/quote/CPNG)    0.286      90.000   \n",
       "2070       [AON](https://finance.yahoo.com/quote/AON)    0.285      20.000   \n",
       "18248      [USB](https://finance.yahoo.com/quote/USB)    0.281       5.000   \n",
       "15587    [ROKU](https://finance.yahoo.com/quote/ROKU)    0.280      30.000   \n",
       "3884       [CAH](https://finance.yahoo.com/quote/CAH)    0.278      10.000   \n",
       "\n",
       "       Quarter Spend  Year Spend  Total Spend  Lobby Accounts  Gov Entities  \n",
       "9573          30.000      30.000      320.000               3             3  \n",
       "5394          40.000      40.000       40.000               4             0  \n",
       "12404         20.000      20.000       20.000               1             5  \n",
       "9059          60.000      60.000       60.000              11             1  \n",
       "13873         60.000     110.000      110.000               5             3  \n",
       "...              ...         ...          ...             ...           ...  \n",
       "5036         880.000    3310.000     6740.000               4             2  \n",
       "2070          70.000     470.000     6395.000               2             2  \n",
       "18248        385.000    1555.000    25678.037               0             0  \n",
       "15587        290.000    1140.000     3090.000               1             2  \n",
       "3884         254.000    1956.000    40545.000               4             2  \n",
       "\n",
       "[677 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d63848ea-5f7c-44ab-854f-1d4bd622bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import locale\n",
    "\n",
    "# Set locale to US English\n",
    "locale.setlocale(locale.LC_TIME, 'en_US.UTF-8')\n",
    "\n",
    "\n",
    "def get_week_ending_label(reference_date=None):\n",
    "    \"\"\"\n",
    "    Returns a formatted string indicating the week ending on the last Friday relative to the reference date.\n",
    "\n",
    "    Args:\n",
    "        reference_date (datetime.date, optional): The date to reference. Defaults to today.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted string like \"Week ending Friday 25th October, 2024\"\n",
    "    \"\"\"\n",
    "    if reference_date is None:\n",
    "        reference_date = datetime.date.today()\n",
    "    \n",
    "    def get_ordinal(n):\n",
    "        if 11 <= n % 100 <= 13:\n",
    "            suffix = 'th'\n",
    "        else:\n",
    "            suffix = {1: 'st', 2: 'nd', 3: 'rd'}.get(n % 10, 'th')\n",
    "        return f\"{n}{suffix}\"\n",
    "    \n",
    "    days_since_friday = (reference_date.weekday() - 4) % 7\n",
    "    last_friday = reference_date - datetime.timedelta(days=days_since_friday)\n",
    "    day_with_ordinal = get_ordinal(last_friday.day)\n",
    "    formatted_date = f\"Week ending {last_friday.strftime('%A')} {day_with_ordinal} {last_friday.strftime('%B')}, {last_friday.year}\"\n",
    "    \n",
    "    return formatted_date\n",
    "\n",
    "# Usage\n",
    "formatted_week_label = get_week_ending_label()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddf7d5c3-2be4-4c1d-8df1-913e43e48a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published Chart URL: [{'id': 'standalone', 'url': 'https://www.datawrapper.de/_/XJieX/', 'name': 'For sharing'}]\n"
     ]
    }
   ],
   "source": [
    "from datawrapper import Datawrapper\n",
    "\n",
    "\n",
    "# Initialize Datawrapper\n",
    "dw = Datawrapper(access_token=\"your_token\")\n",
    "\n",
    "# Create chart\n",
    "chart = dw.create_chart(\n",
    "    title=\"Company Lobbying Activity\",\n",
    "    chart_type=\"tables\"\n",
    ")\n",
    "\n",
    "# Add data\n",
    "dw.add_data(chart['id'], data=final_output)\n",
    "\n",
    "# Configure visualization\n",
    "# Configure visualization\n",
    "# First rename the column in your DataFrame\n",
    "final_output = final_output.rename(columns={'Transactions': 'Trans. Counts',\"Lobbyist Counts\":'Lobby Accounts'})\n",
    "\n",
    "# Updated configuration\n",
    "properties = {\n",
    "    \"visualize\": {\n",
    "        \"dark-mode-invert\": True,\n",
    "        \"columns\": {\n",
    "            \"Ticker\": {\n",
    "                \"align\": \"left\",\n",
    "                \"title\": \"Company\",\n",
    "                \"width\": \"100\",\n",
    "                \"markdown\": True,\n",
    "                \"fixedWidth\": False\n",
    "            },\n",
    "            \"Anomaly\": {\n",
    "                \"style\": {\"fontSize\": 1},\n",
    "                \"title\": \"Anomaly\",\n",
    "                \"width\": 0.45,\n",
    "                \"format\": \"0.0%\",\n",
    "                \"barStyle\": \"slim\",\n",
    "                \"showAsBar\": True,\n",
    "                \"borderLeft\": \"none\",\n",
    "                \"fixedWidth\": True,\n",
    "                \"customBarColor\": False,\n",
    "                \"barColorNegative\": \"#ff4444\",\n",
    "                \"barColorPositive\": \"#44bb77\",\n",
    "                \"customBarColorBy\": \"Ticker\"\n",
    "            },\n",
    "            \"Week Spend\": {\n",
    "                \"title\": \"Week Spend\",\n",
    "                \"width\": \"100\",\n",
    "                \"format\": \"$0,0.0\",\n",
    "                \"fixedWidth\": False\n",
    "            },\n",
    "            \"Quarter Spend\": {\n",
    "                \"title\": \"Quarter Spend\",\n",
    "                \"width\": \"100\",\n",
    "                \"format\": \"$0,0.0\",\n",
    "                \"fixedWidth\": False\n",
    "            },\n",
    "            \"Year Spend\": {\n",
    "                \"title\": \"Year Spend\",\n",
    "                \"width\": \"100\",\n",
    "                \"format\": \"$0,0.0\",\n",
    "                \"fixedWidth\": False\n",
    "            },\n",
    "            \"Total Spend\": {\n",
    "                \"title\": \"Total Spend\",\n",
    "                \"width\": \"100\",\n",
    "                \"format\": \"$0,0.0\",\n",
    "                \"fixedWidth\": False\n",
    "            },\n",
    "            \"Lobby Accounts\": {\n",
    "                \"title\": \"Lobby.\",\n",
    "                \"width\": \"80\",\n",
    "                \"format\": \"0\",\n",
    "                \"fixedWidth\": False,\n",
    "                \"includeInHeatmap\": True\n",
    "            },\n",
    "            \"Gov Entities\": {\n",
    "                \"title\": \"Gov. Ent.\",\n",
    "                \"width\": \"80\",\n",
    "                \"format\": \"0\",\n",
    "                \"fixedWidth\": False,\n",
    "                \"includeInHeatmap\": True\n",
    "            }\n",
    "        },\n",
    "        \"header\": {\n",
    "            \"style\": {\n",
    "                \"bold\": True,\n",
    "                \"color\": \"#494949\",\n",
    "                \"italic\": False,\n",
    "                \"fontSize\": 0.9,\n",
    "                \"background\": False\n",
    "            },\n",
    "            \"borderTop\": \"none\",\n",
    "            \"borderBottom\": \"2px\",\n",
    "            \"borderTopColor\": \"#333333\",\n",
    "            \"borderBottomColor\": \"#333333\"\n",
    "        },\n",
    "        \"heatmap\": {\n",
    "            \"enabled\": True,\n",
    "            \"mode\": \"continuous\",\n",
    "            \"stops\": \"equidistant\",\n",
    "            \"colors\": [\n",
    "                {\"color\": \"#f0f9e8\", \"position\": 0},\n",
    "                {\"color\": \"#b6e3bb\", \"position\": 0.16666666666666666},\n",
    "                {\"color\": \"#75c8c5\", \"position\": 0.3333333333333333},\n",
    "                {\"color\": \"#4ba8c9\", \"position\": 0.5},\n",
    "                {\"color\": \"#2989bd\", \"position\": 0.6666666666666666},\n",
    "                {\"color\": \"#0a6aad\", \"position\": 0.8333333333333334},\n",
    "                {\"color\": \"#254b8c\", \"position\": 1}\n",
    "            ],\n",
    "            \"palette\": 0,\n",
    "            \"rangeMax\": \"13\",\n",
    "            \"rangeMin\": \"1\",\n",
    "            \"stopCount\": 5,\n",
    "            \"hideValues\": False,\n",
    "            \"customStops\": [],\n",
    "            \"rangeCenter\": \"4\",\n",
    "            \"categoryOrder\": [],\n",
    "            \"interpolation\": \"equidistant\",\n",
    "            \"categoryLabels\": {},\n",
    "            \"columns\": [\"Trans. Counts\", \"Lobbyist\", \"Gov Entities\"]  # Specify columns to include in heatmap\n",
    "        },\n",
    "        \"perPage\": 15,\n",
    "        \"striped\": True,\n",
    "        \"markdown\": True,\n",
    "        \"showHeader\": True,\n",
    "        \"compactMode\": True,\n",
    "        \"firstRowIsHeader\": False,\n",
    "        \"firstColumnIsSticky\": True,\n",
    "        \"searchable\": True,\n",
    "        \"search\": {\n",
    "            \"enabled\": True,\n",
    "            \"placeholder\": \"Search companies...\"\n",
    "        }\n",
    "    },\n",
    "    \"describe\": {\n",
    "        \"intro\": (\"Analysis of company lobbying activities. Data shows spending across different time periods (in thousands of dollars), number of transactions, lobbyists involved, and government entities engaged. The anomaly score indicates unusual lobbying activity patterns.\"\n",
    "                 f\" {formatted_week_label}.\"\n",
    "                 \" Derived from <a href='https://docs.sov.ai/realtime-datasets/equity-datasets/lobbying-data/'>Sov.aiâ„¢ Lobbying</a> datasets.\"),\n",
    "        \"byline\": \"\",\n",
    "        \"source-name\": \"Lobbying Data\",\n",
    "        \"hide-title\": False\n",
    "    },\n",
    "    \"publish\": {\n",
    "        \"embed-width\": 700,\n",
    "        \"embed-height\": 714,\n",
    "        \"blocks\": {\n",
    "            \"logo\": {\"enabled\": False},\n",
    "            \"embed\": False,\n",
    "            \"download-pdf\": False,\n",
    "            \"download-svg\": False,\n",
    "            \"get-the-data\": False,\n",
    "            \"download-image\": True\n",
    "        },\n",
    "        \"autoDarkMode\": False,\n",
    "        \"chart-height\": 582,\n",
    "        \"force-attribution\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "# Update and publish chart\n",
    "dw.update_chart(\n",
    "    chart['id'],\n",
    "    metadata=properties\n",
    ")\n",
    "\n",
    "# Publish the chart\n",
    "dw.publish_chart(chart['id'])\n",
    "\n",
    "# Get the published URL\n",
    "published_url = dw.get_chart_display_urls(chart['id'])\n",
    "print(\"Published Chart URL:\", published_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0c82691-a746-4b96-ae29-4b58f6fa0994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 'Predict a Mockingbird - 2024-10-30' already exists. Appending new content to it.\n",
      "New content appended successfully.\n",
      "View your page here: https://www.notion.so/12f094f0f39581b4b996e912a2f7a7f4\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# Define title\n",
    "page_title = \"Predict a Mockingbird\"\n",
    "\n",
    "# Define content sections using the content_sections dictionary\n",
    "content_sections = {\n",
    "    \"section_1\": {\n",
    "        \"heading\": \"Corporate lobbying tracker\",\n",
    "        \"content\": (\n",
    "            \"Lobbiest have important roles to play in the American political system \"\n",
    "            \" and are often responible for many of the bills being passed and the policies implemented.\"\n",
    "            \n",
    "        ),\n",
    "        \"url\": published_url[0][\"url\"],\n",
    "        \"list\": None\n",
    "    }\n",
    "\n",
    "    # Add more sections as needed\n",
    "}\n",
    "\n",
    "# Handle page creation or append\n",
    "handle_page_creation_or_append(page_title, DATABASE_ID, content_sections)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
